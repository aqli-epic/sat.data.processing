---
title: "v1 of the new workflow: to be shared with VIT by January 15th, 2023"
author: "Aarsh"
date: '2022-10-18'
output: html_document
---

# setup
```{r setup}
knitr::opts_chunk$set(echo = FALSE)
# start time
start_time <- Sys.time()

# load libraries
library(raster)
library(rgdal)
library(dplyr)
library(readr)
library(ncdf4)
library(assertthat)
library(fasterize)
library(sf)
library(SpaDES)
library(foster) # for matching resolution of 2 different rasters
library(DBI) # for connecting R with Postgres
library(RPostgres)
library(ggplot2)
library(RPostgres)
library(sparklyr)
library(data.table)
library(stringr)

# global variables (check for updates, if any)
who_pm2.5_standard <- 5 # in micrograms per cubic meter, annual average PM2.5 standard
aqli_lyl_constant <- 0.098
india_pm2.5_standard <- 40 # in micrograms per cubic meter
region_pm2.5_standard <- 15 # China

print("Libraries and Global variables loaded in.")

```

#> Using the new workflow, generate gadm level 2 population weighted pollution and life years lost numbers (WHO and National Standard) for a given year's Global data. Standalone, uses its own data files, not the data files from the second chunk up top.

# set paths

```{r set_paths, echo=FALSE}

#> paths and global variables (create the necessary folder structure after reading through the paths section. All paths are relative to your current working directory, but you might need to make a few folders, for e.g. for specific resolution datasets). After that, run this script and everything should run smoothly.----------------------------------------

# pollution
pol_data_location <-"./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/"



# population
pop_data_location <- "./ar.2023.update.using.2021.pol.data/data/input/population/"
pop_data_file_name <- "landscan-global-2021.tif"



# shapefiles
shp_files_location <- "./ar.2023.update.using.2021.pol.data/data/intermediate/1_population_and_colormap/1_shapefile_aggregate/"

colormap_location <- "colormap/colormap.shp"

hovermap_location <- "hover/hover.shp"

# raster resolution of the final data brick (containing a rasterized pollution, population and a rasterized shape file)
raster_res <- 0.008

# data timeline (note)
pol_data_start_year <- 1998
update_year <- 2021

# data levels
gadm0_folder_name <- "gadm_0"
gadm1_folder_name <- "gadm_1"
gadm2_folder_name <- "gadm_2"

# corresponnding aqli report publishing year (this is the year in which "update_year"'s data will be published. Current lag is 2 years).
report_publishing_year <- 2023

# ssd aqli folder high res (0.01 as in December, 2022) location (use ssd for high writing speeds, hdd's suck)

# drive location
ssd_drive <- "D:/"

# data folder name (on ssd)
aqli_data_share_folder_name <- "aqli.2023.report.data.share"

# high res data location
ssd_location_rasterized_data_0.008 <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".data", "/rasterized/", raster_res, "/", sep = "")

#> collapsed data path: gadm0 level
ssd_location_collapsed_gadm0_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm0_folder_name, "/", sep  = "")

#> collapsed data path: gadm1 level
ssd_location_collapsed_gadm1_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm1_folder_name, "/", sep  = "")

#> collapsed data path: gadm2 level
ssd_location_collapsed_gadm2_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm2_folder_name, "/", sep  = "")


#-------------------------------------------------

```

# main pipeline to get yearly gadm2 and high res pollution datasets

```{r get_yearly_pol_datasets, echo=FALSE}

# benchmarking
threshold_0 <- Sys.time() 

# population raw data
population_dataset <- raster::raster(str_c(pop_data_location, pop_data_file_name, sep = ""))

# naming the raw global landscan population data and setting its crs to be the same as the pollution data
crs(population_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(population_dataset) <- "population"


#--- no longer need to crop, as this can be reused, so I wrote it as a tif, and then will simply read it
#- crop population dataset to the pollution dataset
# pop_raw_landscan_crop_pol <- raster::crop(population_dataset, pollution_dataset)

#- write the cropped population dataset
# pop_raw_landscan_crop_pol %>%
#   raster::writeRaster(filename = "./experimentation/pop_raw_landscan_crop_pol.tif", 
#                       format = "GTiff", overwrite = TRUE)
#---

# reading in the pre-cropped population raster, which remains the same for all pollution datasets
pop_raw_landscan_crop_pol <- raster::raster("./experimentation/pop_raw_landscan_crop_pol.tif")


# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

#-- this remains same for all pollution datasets, hence writing it, and will then simply read it
# polygon_cells <- fasterize(colormap, pol_0.01_region_in_landscan_pop_res, field = "objectid", fun = "last")
# writeRaster(polygon_cells,
# 	filename = "./experimentation/colormap_rasterized.tif",
# 	format = "GTiff", overwrite = TRUE)
#---

polygon_cells <- raster::raster("./experimentation/colormap_rasterized.tif")


# benchmarking
threshold_1 <- Sys.time() 


#> reading pollution data, one year at a time and then will concatenate all results-----------------------
# Note that the datasets in this pipeline do not include the  geometry column. That can be added in the end after concatenating all yearly datasets into a single final gadm2 dataset. For the gadm0 and gadm1 datasets (which will be derived from the single "final gadm2 dataset"), a similar process will follow.

# pol data list
pol_data_list <- list.files(pol_data_location) %>% sort()

# pollution column names empty vector
pol_col_name_vec <- c()

#> for loop begins----------------------------------------------------------------

for (i in 1:length(pol_data_list)){
  
  print(stringr::str_c("Iteration #", i, " begins"))
  
  pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
  
  # for testing purposes
  if(i == 5){
    break
  }
  
  # pollution file name given the current iteration
  cur_pol_file_name <- pol_data_list[i]
  
  # cur pollution file year
  cur_pol_file_year <- stringr::str_extract(str_extract(cur_pol_file_name, "(\\d+)-(\\d+)"), "....")
  
  # read in the pollution raster for a given year
  pollution_dataset <- raster::raster(str_c(pol_data_location, cur_pol_file_name, sep = ""))
  
  # naming the raw global pollution data and setting its crs
crs(pollution_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(pollution_dataset) <- "pm2.5_pollution"


# benchmarking
threshold_2 <- Sys.time() 

# matching the resolution of the cropped population and pollution datasets
pol_0.01_region_in_landscan_pop_res <- foster::matchResolution(pollution_dataset, pop_raw_landscan_crop_pol)

# benchmarking
threshold_3 <- Sys.time() 

print("stacking all layers in a raster brick")

# creating a raster brick using the population and pollution data
region_raster_brick <- pop_raw_landscan_crop_pol %>% 
  raster::addLayer(pol_0.01_region_in_landscan_pop_res) 

# setting the names of the newly created placheolders
names(region_raster_brick) <- c("population", "pm2.5_pollution")

# set the same crs for pollution brick
crs(region_raster_brick) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

print("Population and pollution layers matched")

# benchmarking
threshold_4 <- Sys.time() 

# Now match each population/pollution point to a colormap polygon. To do this, convert
# polygons to raster of same resolution as population raster, with value of each cell equal
# to objectid of polygon that covers its center.
# Fasterize is an ultra-fast version of the rasterize function.

# add rasterized colormap to the raster brick
region_raster_brick <- region_raster_brick %>% 
  raster::addLayer(polygon_cells)

print("added rasterized colormap layer to the brick")

names(region_raster_brick)[length(names(region_raster_brick))] <- "colormap_objectid"

# benchmarking
threshold_5 <- Sys.time() 

# from this point forward, replace all instances of "region_raster_brick_df" with "aqli_raster_brick_df". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_df <- raster::as.data.frame(region_raster_brick)

print("raster brick converted to data frame")

# write rasterized dataframe to ssd, in arrow data format (0.008x0.008 resolution)
aqli_raster_brick_df %>% arrow::write_dataset(str_c(ssd_location_rasterized_data_0.008, cur_pol_file_year, ".parquet"))

# benchmarking
threshold_6 <- Sys.time() 

#-- (Update: no longer needed as we directly coerce region_raster_brick_df to an arrow table). Will only need to write this when implementing high res layer.
#- write region_raster_brick_df to a csv
#--

# from this point forward, replace all instances of "pollution_data_0.01_light" with "aqli_raster_brick_light". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_light <- arrow::as_arrow_table(aqli_raster_brick_df)

# benchmarking
threshold_7 <- Sys.time() 

print("rasterized dataframe coerced to an arrow table")

#-- read raster data using arrow (no longer needed as we already coerced region_raster_brick_df to an arrow table)
# aqli_raster_brick_light <- arrow::open_dataset("./experimentation/pollution_data_0.01_2021.csv", format = "csv")
#-- 


# from this point forward, replace all instances of "pollution_district_wise" with "aqli_gadm2_collapse". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_gadm2_collapse <- aqli_raster_brick_light %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE), 
            lyl_rel_who = round((avg_pm2.5_pollution - who_pm2.5_standard)*aqli_lyl_constant, 2), 
            lyl_rel_who = ifelse(lyl_rel_who < 0, 0, lyl_rel_who)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# renaming the pollution column such that it includes the year in question
colnames(aqli_gadm2_collapse)[str_detect(colnames(aqli_gadm2_collapse), "avg_pm2.5_pollution")] <- pol_col_name_vec[i]

# writing the gadm2 level dataset to ssd
aqli_gadm2_collapse %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, cur_pol_file_year, "_", gadm2_folder_name, ".csv"))

# benchmarking
threshold_8 <- Sys.time() # r1 (t8 to t9: 6.89 minutes), r2 (t8 to t9: 4.53 minutes)



print(str_c("Iteration #", i, " ends"))
 
}

end_time_main_pipeline <- Sys.time()
time_diff <- end_time_main_pipeline - start_time
print(str_c("Time taken (main pipeline for loop): ", time_diff, " minutes"))

#> for loop ends-----------------------------------------------------------------------


```

# combine all pollution yearly datasets into a single gadm2 dataset, in the format that has to be shared with VIT. See AQLI data dictionary for more information.

```{r combine_yearly_pol_data, echo=FALSE}

# benchmarking
threshold_9 <- Sys.time() 

#> Output in VIT data sharing format

# read in all pollution data and name_0, name_1, name_2 columns from colormap (joined in the first iteration of the loop below) into a single dataset, with just pollution columns. I am in the process of updating national standards, so for now we have a placeholder national standards column, which is just set to 10 micrograms per cubic meter for all regions.

collapsed_gadm2_files_vec <- list.files(str_c(ssd_location_collapsed_gadm2_data_path)) %>% 
  sort()

for(i in 1:length(collapsed_gadm2_files_vec)){
  if(i == 1){
    temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec[i]))
    
    aqli_gadm2_collapse_master <- temp_gadm2 %>%
      dplyr::left_join(colormap, by = c("objectid_gadm2" = "objectid")) %>%
      mutate(whostandard = 5, 
             natstandard = 10) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, NAME_0, NAME_1, NAME_2, total_population,
             whostandard, natstandard,
             pol_col_name_vec[i]) %>%
      dplyr::rename(country = NAME_0,
             name_1 = NAME_1, 
             name_2 = NAME_2,
             population = total_population)
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
    
  } else{
     temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec[i]))
    
    aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>% 
      dplyr::left_join(temp_gadm2, by  = "objectid_gadm2") %>%
      dplyr::select(-c(total_population, lyl_rel_who)) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, pol_col_name_vec[1:(i-1)],
             pol_col_name_vec[i])
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
  
  }
}

# benchmarking
threshold_10 <- Sys.time() 

# adding in the life years lost columns

aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, everything()) 

# removing the substring "pm" from life expectancy column names
colnames(aqli_gadm2_collapse_master)[str_detect(colnames(aqli_gadm2_collapse_master), "llpp")] <- stringr::str_remove(colnames(aqli_gadm2_collapse_master)[stringr::str_detect(colnames(aqli_gadm2_collapse_master), "llpp")], "pm")


# round all pm and lyl columns to two decimal places
aqli_gadm2_collapse_master[, stringr::str_detect(colnames(aqli_gadm2_collapse_master), "llpp|pm")] <- aqli_gadm2_collapse_master %>%
  dplyr::select(-c(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard)) %>%
  purrr::map_df(round, 2) 

# benchmarking
threshold_11 <- Sys.time() 

# write the above (non-geometry version) of gadm2 master file to the collapsed/gadm2 folder

aqli_gadm2_collapse_master %>%
  write_csv(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_non_geom.csv"))

# add in the geometry column by left joining the above with a color file and then write that to the collapsed/gadm2 folder

aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master %>%
  left_join(colormap %>% rename(objectid_gadm2 = objectid), by = "objectid_gadm2") %>%
  rename(iso_alpha3 = iso_alpha3.x) %>%
  select(-c(iso_alpha3.y, NAME_0, NAME_1, NAME_2)) %>%
  st_as_sf()

# aqli_gadm2_collapse_master_with_geom %>%
#   st_write(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_with_geom.shp"))

# collapse the gadm2 file to gadm1 (state) level and write both geom and non-geom versions to their respective folders

#tbd

# collapse the gadm2 file to gadm0 (country) level and write both geom and non-geom versions to their respective folders


#tbd

end_time_vit_data_pipeline <- Sys.time()
print(str_c("Total time taken: ", end_time_vit_data_pipeline - start_time,  " minutes?"))
```


# Sanity checks area (deactivated for now)
```{r sanity_checks, eval=FALSE, include=FALSE}
#> sanity checks time-------------------------------------------------------------

# 1: number of NA's in pollution for each colormap object id
foo <- arrow::open_dataset("D:/aqli.202.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/1999.parquet/part-0.parquet")

aqli_gadm2_collapse_na_summary <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 
  
# 2: number of object ids for which we do not have any data
obj_id_missing <- which(colormap$objectid %notin% unique(aqli_gadm2_collapse_na_summary$colormap_objectid))

colormap %>%
  filter(objectid %in% obj_id_missing) %>% 
  st_write("./experimentation/missing_obj_ids.shp")

# 3: number of NAs in population for each colormap object id
aqli_gadm2_collapse_na_summary_pop <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pop = sum(is.na(population)), 
    prop_na = round((total_nas_pop/count_rows)*100, 2)) 

#-------------------------------------------------------------------------------
```
