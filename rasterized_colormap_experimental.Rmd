---
title: "v1 of the new workflow: to be shared with VIT by January 15th, 2023"
author: "Aarsh (aarshbatra@uchicagotrust.org/aarshbatra.in@gmail.com)"
date: '2022-10-18'
output: html_document
---

# setup
```{r setup}
knitr::opts_chunk$set(echo = FALSE)
# start time
start_time <- Sys.time()

# load libraries
library(raster)
library(rgdal)
library(dplyr)
library(readr)
library(ncdf4)
library(assertthat)
library(fasterize)
library(sf)
library(SpaDES)
library(foster) # for matching resolution of 2 different rasters
library(DBI) # for connecting R with Postgres
library(RPostgres)
library(ggplot2)
library(RPostgres)
# library(sparklyr)
library(data.table)
library(stringr)

# global variables (check for updates, if any)
who_pm2.5_standard <- 5 # in micrograms per cubic meter, annual average PM2.5 standard
aqli_lyl_constant <- 0.098
india_pm2.5_standard <- 40 # in micrograms per cubic meter
region_pm2.5_standard <- 15 # China

print("Libraries and Global variables loaded in.")

```

#> Using the new workflow, generate gadm level 2 population weighted pollution and life years lost numbers (WHO and National Standard) for a given year's Global data. Standalone, uses its own data files, not the data files from the second chunk up top.

# set paths

```{r set_paths, echo=FALSE}

#> paths and global variables (create the necessary folder structure after reading through the paths section. All paths are relative to your current working directory, but you might need to make a few folders, for e.g. for specific resolution datasets). After that, run this script and everything should run smoothly.----------------------------------------

# pollution
pol_data_location <-"./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/"



# population
pop_data_location <- "./ar.2023.update.using.2021.pol.data/data/input/population/"
pop_data_file_name <- "landscan-global-2021.tif"



#> shapefiles

# general shape file folder location
shp_files_location <- "./ar.2023.update.using.2021.pol.data/data/intermediate/1_population_and_colormap/1_shapefile_aggregate/"

# gadm2 shape file location
colormap_location <- "colormap/colormap.shp"

# hover map shape file location (not using this year)
hovermap_location <- "hover/hover.shp"

# gadm0 shape file location
gadm0_shp_file_location <- "colormap_collapsed_gadm0/aqli_gadm2_collapse_to_gadm0.shp" 

# gadm1 shape file location
gadm1_shp_file_location <- "colormap_collapsed_gadm1/aqli_gadm2_collapse_to_gadm1.shp" 


#> raster resolution of the final data brick (containing a rasterized pollution, population and a rasterized shape file)
raster_res <- 0.008

# data timeline (note)
pol_data_start_year <- 1998
update_year <- 2021

# data levels
gadm0_folder_name <- "gadm_0"
gadm1_folder_name <- "gadm_1"
gadm2_folder_name <- "gadm_2"

# corresponnding aqli report publishing year (this is the year in which "update_year"'s data will be published. Current lag is 2 years).
report_publishing_year <- 2023

# ssd aqli folder high res (0.01 as in December, 2022) location (use ssd for high writing speeds, hdd's suck)

# drive location
ssd_drive <- "D:/"

# data folder name (on ssd)
aqli_data_share_folder_name <- "aqli.2023.report.data.share"

# high res data location
ssd_location_rasterized_data_0.008 <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".data", "/rasterized/", raster_res, "/", sep = "")

# updated national standards file location and file name
national_standards_pm2.5_jan_2023_location <- "./ar.2023.update.using.2021.pol.data/data/input/standards/" 

national_standards_pm2.5_jan_2023_file_name <- "country_annual_average_pm2.5_standards_asInJan2023.csv"

#> collapsed data path: gadm0 level
ssd_location_collapsed_gadm0_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm0_folder_name, "/", sep  = "")

#> collapsed data path: gadm1 level
ssd_location_collapsed_gadm1_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm1_folder_name, "/", sep  = "")

#> collapsed data path: gadm2 level
ssd_location_collapsed_gadm2_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm2_folder_name, "/", sep  = "")


#-------------------------------------------------

```

# main pipeline to get yearly gadm2 and high res pollution datasets

```{r get_yearly_pol_datasets, echo=FALSE}

# benchmarking
threshold_0 <- Sys.time() 

# population raw data
population_dataset <- raster::raster(str_c(pop_data_location, pop_data_file_name, sep = ""))

# naming the raw global landscan population data and setting its crs to be the same as the pollution data
raster::crs(population_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(population_dataset) <- "population"


#--- no longer need to crop, as this can be reused, so I wrote it as a tif, and then will simply read it
#- crop population dataset to the pollution dataset
# pop_raw_landscan_crop_pol <- raster::crop(population_dataset, pollution_dataset)

#- write the cropped population dataset
# pop_raw_landscan_crop_pol %>%
#   raster::writeRaster(filename = "./experimentation/pop_raw_landscan_crop_pol.tif", 
#                       format = "GTiff", overwrite = TRUE)
#---

# reading in the pre-cropped population raster, which remains the same for all pollution datasets
pop_raw_landscan_crop_pol <- raster::raster("./experimentation/pop_raw_landscan_crop_pol.tif")


# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

#-- this remains same for all pollution datasets, hence writing it, and will then simply read it
# polygon_cells <- fasterize(colormap, pol_0.01_region_in_landscan_pop_res, field = "objectid", fun = "last")
# writeRaster(polygon_cells,
# 	filename = "./experimentation/colormap_rasterized.tif",
# 	format = "GTiff", overwrite = TRUE)
#---

polygon_cells <- raster::raster("./experimentation/colormap_rasterized.tif")

# making sure that the crs of the rasterized shapefile is the same as the 0.008 population and pollution rasters. 
raster::crs(polygon_cells) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

# benchmarking
threshold_1 <- Sys.time() 

print("All raw datasets read into R.")

#> reading pollution data, one year at a time and then will concatenate all results-----------------------
# Note that the datasets in this pipeline do not include the  geometry column. That can be added in the end after concatenating all yearly datasets into a single final gadm2 dataset. For the gadm0 and gadm1 datasets (which will be derived from the single "final gadm2 dataset"), a similar process will follow.

# pol data list
pol_data_list <- list.files(pol_data_location) %>% sort()

# pollution column names empty vector
pol_col_name_vec <- c()

#> for loop begins----------------------------------------------------------------

print("For loop begins: Processing pollution rasters 1 year at a time")

for (i in 1:length(pol_data_list)){
  
  # benchmarking
  threshold_1.5 <- Sys.time()
  
  print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " begins"))
  
  pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
  
  # for testing purposes
  if(i < 12){
    next
  }
  
  # if(i > 10){
  #   break
  # }
  
  # pollution file name given the current iteration
  cur_pol_file_name <- pol_data_list[i]
  
  # cur pollution file year
  cur_pol_file_year <- stringr::str_extract(str_extract(cur_pol_file_name, "(\\d+)-(\\d+)"), "....")
  
  # read in the pollution raster for a given year
  pollution_dataset <- raster::raster(str_c(pol_data_location, cur_pol_file_name, sep = ""))
  
  # naming the raw global pollution data and setting its crs
raster::crs(pollution_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(pollution_dataset) <- "pm2.5_pollution"


# benchmarking
threshold_2 <- Sys.time() 

# matching the resolution of the cropped population and pollution datasets
pol_0.01_region_in_landscan_pop_res <- foster::matchResolution(pollution_dataset, pop_raw_landscan_crop_pol)

# benchmarking
threshold_3 <- Sys.time() 

print("stacking all layers in a raster brick")

# creating a raster brick using the population and pollution data
region_raster_brick <- pop_raw_landscan_crop_pol %>% 
  raster::addLayer(pol_0.01_region_in_landscan_pop_res) 

# setting the names of the newly created placheolders
names(region_raster_brick) <- c("population", "pm2.5_pollution")

# set the same crs for pollution brick
raster::crs(region_raster_brick) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

print("Population and pollution layers matched")

# benchmarking
threshold_4 <- Sys.time() 

# Now match each population/pollution point to a colormap polygon. To do this, convert
# polygons to raster of same resolution as population raster, with value of each cell equal
# to objectid of polygon that covers its center.
# Fasterize is an ultra-fast version of the rasterize function.

# add rasterized colormap to the raster brick
region_raster_brick <- region_raster_brick %>% 
  raster::addLayer(polygon_cells)

print("added rasterized colormap layer to the brick")

names(region_raster_brick)[length(names(region_raster_brick))] <- "colormap_objectid"

# benchmarking
threshold_5 <- Sys.time() 

# from this point forward, replace all instances of "region_raster_brick_df" with "aqli_raster_brick_df". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_df <- raster::as.data.frame(region_raster_brick)

print("raster brick converted to data frame")

# write rasterized dataframe to ssd, in arrow data format (0.008x0.008 resolution)
aqli_raster_brick_df %>% arrow::write_dataset(str_c(ssd_location_rasterized_data_0.008, cur_pol_file_year, ".parquet"))

# benchmarking
threshold_6 <- Sys.time() 

#-- (Update: no longer needed as we directly coerce region_raster_brick_df to an arrow table). Will only need to write this when implementing high res layer.
#- write region_raster_brick_df to a csv
#--

# from this point forward, replace all instances of "pollution_data_0.01_light" with "aqli_raster_brick_light". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_light <- arrow::as_arrow_table(aqli_raster_brick_df)

# benchmarking
threshold_7 <- Sys.time() 

print("rasterized dataframe coerced to an arrow table")

#-- read raster data using arrow (no longer needed as we already coerced region_raster_brick_df to an arrow table)
# aqli_raster_brick_light <- arrow::open_dataset("./experimentation/pollution_data_0.01_2021.csv", format = "csv")
#-- 


# from this point forward, replace all instances of "pollution_district_wise" with "aqli_gadm2_collapse". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_gadm2_collapse <- aqli_raster_brick_light %>%
  dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# renaming the pollution column such that it includes the year in question
colnames(aqli_gadm2_collapse)[str_detect(colnames(aqli_gadm2_collapse), "avg_pm2.5_pollution")] <- pol_col_name_vec[i]

# writing the gadm2 level dataset to ssd
aqli_gadm2_collapse %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, cur_pol_file_year, "_", gadm2_folder_name, ".csv"))

# benchmarking
threshold_8 <- Sys.time() 



print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " end"))

print("Freeing memory before proceeding to next iteration")

# benchmarking
threshold_9 <- Sys.time()

# collect garbage, free memory
gc()

# benchmarking
threshold_10 <- Sys.time()

# time it took to complete the current iteration

print(str_c("Time taken to complete iteration # ", i, "/", (update_year - pol_data_start_year) + 1, ": ", threshold_10 - threshold_1.5))

}

end_time_main_pipeline <- Sys.time()
time_diff <- end_time_main_pipeline - start_time

print(str_c("Total time taken (main pipeline for loop) with ", i, " years of pollution data processed: ", time_diff))

#> for loop ends-----------------------------------------------------------------------


```

# combine all pollution yearly datasets into a single gadm2 dataset, in the format that has to be shared with VIT. See AQLI data dictionary for more information.

```{r combine_yearly_pol_data, echo=FALSE}

#---------
#> Note: if you have already processed the pollution datasets in the above chunk and just want to run the combining code in this chunk, make sure to uncomment the following code and run it before proceeding to get the colormap and pol_col_name_vec. Also, make sure to run the "setup" and "set path" chunks. If you have already run the above chunks, then you would already have these objects and in that case there is no need to run the below commented code.

# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

# generate the pol_col_name_vec
pol_col_name_vec <- c()
for(i in 1:24){
     pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
 }

#-----------

# benchmarking
threshold_11 <- Sys.time() 

#> Output in VIT data sharing format

# read in all pollution data and name_0, name_1, name_2 columns from colormap (joined in the first iteration of the loop below) into a single dataset, with just pollution columns. I am in the process of updating national standards, so for now we have a placeholder national standards column, which is just set to 10 micrograms per cubic meter for all regions. The updated national standards column will be added alongside the life years lost columns after the below for loop.

# list of files in the collapsed folder
collapsed_gadm2_files_vec <- list.files(str_c(ssd_location_collapsed_gadm2_data_path)) %>% 
  sort() 

# indices of the relevant files (from the above vector) needed for combining.
collapsed_gadm2_files_vec_rel_ind <- str_detect(collapsed_gadm2_files_vec, ".csv")

# keeping only relevant files that will be combined below
collapsed_gadm2_files_vec_rel_files <- collapsed_gadm2_files_vec[collapsed_gadm2_files_vec_rel_ind]

for(i in 1:length(collapsed_gadm2_files_vec_rel_files)){
  if(i == 1){
    temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))
    
    aqli_gadm2_collapse_master <- temp_gadm2 %>%
      dplyr::left_join(colormap, by = c("objectid_gadm2" = "objectid")) %>%
      dplyr::mutate(whostandard = 5, 
             natstandard = 10) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, NAME_0, NAME_1, NAME_2, total_population,
             whostandard, natstandard,
             pol_col_name_vec[i]) %>%
      dplyr::rename(country = NAME_0,
             name_1 = NAME_1, 
             name_2 = NAME_2,
             population = total_population)
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
    
  } else{
     temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))
    
    aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>% 
      dplyr::left_join(temp_gadm2, by  = "objectid_gadm2") %>%
      dplyr::select(-c(total_population)) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, pol_col_name_vec[1:(i-1)],
             pol_col_name_vec[i])
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
  
  }
}

# benchmarking
threshold_12 <- Sys.time() 

#> add in the updated national standards column (last updated: January 18, 2023 by Aarsh) and bring it in appropriate "join ready" format (to be joined with the combined pollution data before adding life years lost columns)

# reading in the national standards file
national_standards_pm2.5 <- readr::read_csv(stringr::str_c(national_standards_pm2.5_jan_2023_location, national_standards_pm2.5_jan_2023_file_name, sep = ""))

# keeping and renaming relevant columns
national_standards_pm2.5 <- national_standards_pm2.5 %>%
  dplyr::select(country, natstandard_pm2.5_new_2023_report_micr_grm_cubic_meter_op1) %>%
  dplyr::rename(natstandard_updated = natstandard_pm2.5_new_2023_report_micr_grm_cubic_meter_op1)

# replacing the national standards column of the aqli_gadm_collapse_master file (output of the above for loop) with the updated national standards column
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::left_join(national_standards_pm2.5, by = "country") %>%
  dplyr::select(-c(natstandard)) %>%
  dplyr::select(objectid_gadm2:whostandard, natstandard_updated, dplyr::everything()) %>%
  dplyr::rename(natstandard = natstandard_updated)

# replace natstandard == "No national standard", with natstandard == 0, and then changing its data type to "numeric"
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(natstandard = ifelse(natstandard == "No national standard", 0, natstandard))

aqli_gadm2_collapse_master$natstandard <- as.numeric(aqli_gadm2_collapse_master$natstandard)
  
# adding in the life years lost columns and doing some basic cleaning of column names to bring it into the format that we want, as per the AQLI data dictionary

aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, everything()) %>%
    dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))

# benchmarking
threshold_13 <- Sys.time() 

# write the above (non-geometry version) of gadm2 master file to the collapsed/gadm2 folder

aqli_gadm2_collapse_master %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_non_geom_Jan192023.csv"))

#> add in the geometry column by left joining the above with a color file and then write that to the collapsed/gadm2 folder

# joining the gadm2 collapse master file with colormap (using objectid as the joining column), to get the geometry column
aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master %>%
  dplyr::left_join(colormap %>% rename(objectid_gadm2 = objectid), by = "objectid_gadm2") %>%
  dplyr::rename(iso_alpha3 = iso_alpha3.x) %>%
  dplyr::select(-c(iso_alpha3.y, NAME_0, NAME_1, NAME_2))

# creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm2 collapse master dataset, to avoid any confusions.

new_col_names_gadm2_collapse_master_geom_ver <- colnames(aqli_gadm2_collapse_master_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm2", "objidgadm2"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()

# assigning new colnames to the geom version of the gadm2 collapse master file
colnames(aqli_gadm2_collapse_master_with_geom) <- new_col_names_gadm2_collapse_master_geom_ver

# making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file
aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master_with_geom %>%
  sf::st_as_sf() 

aqli_gadm2_collapse_master_with_geom %>%
  sf::write_sf(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_with_geom_Jan192023.shp"))

#-----------


#> collapse the gadm2 file to gadm0 (country) level and write both geom and non-geom versions to their respective folders

# finalizing the gadm0 level dataset
aqli_gadm0_collapse_from_gadm2 <- aqli_gadm2_collapse_master %>%
  dplyr::group_by(country) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE)) %>%
  dplyr::mutate(across(dplyr::starts_with("pm"), ~(.x*pop_weights), .names = "{col}_pop_weighted")) %>%
  dplyr::summarise(across(dplyr::contains("pop_weighted"), ~(round(sum(.x, na.rm = TRUE), 2)), .names = "avg_{col}"),
                   total_population = sum(population, na.rm = TRUE), objectid_gadm2 = objectid_gadm2[1], iso_alpha3 = iso_alpha3[1], whostandard = whostandard[1], natstandard = natstandard[1]) %>%
  select(objectid_gadm2, iso_alpha3, country, total_population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::mutate(objectid_gadm2 = dplyr::row_number()) %>%
  dplyr::rename(population = total_population, 
                objectid_gadm0 = objectid_gadm2) %>%
  dplyr::rename_with(~str_replace_all(.x, "(_pop_weighted)|(avg_)", ""), dplyr::contains("_pop_weighted")) %>%
    dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm0, iso_alpha3, country, population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))
  

# write the above (non-geometry version) of gadm0 master file to the collapsed/gadm0 folder
aqli_gadm0_collapse_from_gadm2 %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_non_geom_Jan192023.csv"))


# read in the gadm0 shape file (which is collapsed from the colormap) and get it ready for joining with the above summary table

gadm0_shp_file <- sf::st_read(str_c(shp_files_location, gadm0_shp_file_location, sep = ""))

gadm0_shp_file_subset <- gadm0_shp_file %>%
  dplyr::select(-c(NAME_1, NAME_2, objectid, iso_alpha3))

# add in the geometry column by left joining the above with a country level shape file and then write that to the collapsed/gadm0 folder

# joining the gadm0 collapse master file with the country level shapefile (using "country/NAME_0" as the joining column), to get the geometry column

aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2 %>%
  dplyr::left_join(gadm0_shp_file_subset, by = c("country" = "NAME_0")) %>%
  sf::st_as_sf()

# (tbd): creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm0 collapse master dataset, to avoid any confusions.

new_col_names_gadm0_collapse_from_gadm2 <- colnames(aqli_gadm0_collapse_from_gadm2_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm0", "objidgadm0"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()


# (tbd): assigning new colnames to the geom version of the gadm0 collapse master file
colnames(aqli_gadm0_collapse_from_gadm2_with_geom) <- new_col_names_gadm0_collapse_from_gadm2

# (tbd): making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file

aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2_with_geom %>%
  sf::st_as_sf()

aqli_gadm0_collapse_from_gadm2_with_geom %>%
  sf::st_write(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_with_geom_Jan192023_.shp"), delete_layer = TRUE)

#-----------

#>  collapse the gadm2 file to gadm1 (state/province) level and write both geom and non-geom versions to their respective folders

# finalizing the gadm1 level dataset
aqli_gadm1_collapse_from_gadm2 <- aqli_gadm2_collapse_master %>%
  dplyr::group_by(country, name_1) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE)) %>%
  dplyr::mutate(across(dplyr::starts_with("pm"), ~(.x*pop_weights), .names = "{col}_pop_weighted")) %>%
  dplyr::summarise(across(dplyr::contains("pop_weighted"), ~(round(sum(.x, na.rm = TRUE), 2)), .names = "avg_{col}"),
                   total_population = sum(population, na.rm = TRUE), objectid_gadm2 = objectid_gadm2[1], iso_alpha3 = iso_alpha3[1], whostandard = whostandard[1], natstandard = natstandard[1]) %>%
  select(objectid_gadm2, iso_alpha3, country, name_1, total_population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::mutate(objectid_gadm2 = dplyr::row_number()) %>%
  dplyr::rename(population = total_population, 
                objectid_gadm1 = objectid_gadm2) %>%
  dplyr::rename_with(~str_replace_all(.x, "(_pop_weighted)|(avg_)", ""), dplyr::contains("_pop_weighted")) %>%
    dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm1, iso_alpha3, country, name_1, population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))
  

# write the above (non-geometry version) of gadm1 master file to the collapsed/gadm1 folder
aqli_gadm1_collapse_from_gadm2 %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_non_geom_Jan192023.csv"))

# read in the gadm1 shape file (which is collapsed from the colormap) and get it ready for joining with the above summary table

gadm1_shp_file <- sf::st_read(str_c(shp_files_location, gadm1_shp_file_location, sep = ""))


gadm1_shp_file_subset <- gadm1_shp_file %>%
  dplyr::select(-c(NAME_2, objectid, iso_alpha3))

# (tbd) add in the geometry column by left joining the above with a state level shape file and then write that to the collapsed/gadm1 folder

# joining the gadm1 collapse master file with the state level shapefile (using " " as the joining column), to get the geometry column

aqli_gadm1_collapse_from_gadm2_with_geom <- aqli_gadm1_collapse_from_gadm2 %>%
  dplyr::left_join(gadm1_shp_file_subset, by = c("country" = "NAME_0", "name_1" = "NAME_1")) %>%
  sf::st_as_sf()


# (tbd): creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm1 collapse master dataset, to avoid any confusions.

new_col_names_gadm1_collapse_from_gadm2 <- colnames(aqli_gadm1_collapse_from_gadm2_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm1", "objidgadm1"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()

# (tbd): assigning new colnames to the geom version of the gadm1 collapse master file
colnames(aqli_gadm1_collapse_from_gadm2_with_geom) <- new_col_names_gadm1_collapse_from_gadm2

# (tbd): making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file

aqli_gadm1_collapse_from_gadm2_with_geom <- aqli_gadm1_collapse_from_gadm2_with_geom %>%
  sf::st_as_sf()

aqli_gadm1_collapse_from_gadm2_with_geom %>%
  sf::st_write(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_with_geom_Jan192023.shp"), delete_layer = TRUE)

#-----------
end_time_vit_data_pipeline <- Sys.time()
print(str_c("Total time taken: ", end_time_vit_data_pipeline - start_time,  " minutes?"))
```



#> Capturing the missing pollution regions and the NA population regions that were left out in the process above because there tiny size.---------------------------------------

#Specifically, in the AQLI 2023 update, there were 153 missing pollution regions, of which 70 were expected to be missing because they were either below 54.995 S or above 67.995 N. Remaining 83 regions were probably extremely small in size (there area being less than the higest available resolution, i.e. 0.01 degree^2, which is ~1km^2), such that when the shapefile was rasterized, these region polygons were so small that no pixel's center lay in them post rasterizing and hence they did not show up in the final list. The rasterizing algorithm works by assigning the pixel in the rasterized version the unique id of the underlying polygon it belongs to, but for that it uses pixel's center to see which underlying polygon that pixel belongs to. To resolve this, for these 153 missing regions we will be rasterizing the shapefile to a resolution of 0.001 degree^2, so that we can capture them. In doing so, it's important to know that interpolating population to higher resoltions doesn't work, so instead we have to interpolate population densities.

# Apart from this there were 97 regions that were captured in the rasterization process, but there population was 0/NA. A similar process would be applied to them.

# Hopefully, once this process is complete all of the regions would be captured. If not, you can try resampling to an even higher resolution, e.g. 0.0001 (10mx10m). But, that would be extremely memory intensive.


```{r}

#> create a vector of objectids where population is NA

# read in the gadm level 2 file
gadm2_aqli_2021 <- readr::read_csv("./ar.2023.update.using.2021.pol.data/data/output_vit/gadm2_aqli2021_vit.csv")

# NA pop regions
objectid_gadm2_na_pop <- gadm2_aqli_2021 %>%
  filter(is.na(population)) %>%
  pull(objectid_gadm2) %>%
  as.vector()

```













# Sanity checks area (deactivated for now)
```{r sanity_checks, eval=FALSE, include=FALSE}
#> sanity checks time-------------------------------------------------------------

#> 1: number of NA's in pollution for each colormap object id
foo <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/1999.parquet/part-0.parquet")

# reading in the raw pollution raster for 1998
pol_raw_1998_tmp <- raster::raster(str_c(pol_data_location, "V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.199801-199812.nc"))

pol_raw_2021_tmp <- raster::raster(str_c(pol_data_location, "V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.202101-202112.nc"))

val_pol_raw_1998_tmp <- values(pol_raw_1998_tmp)
val_pol_raw_2021_tmp <- values(pol_raw_2021_tmp)


aqli_gadm2_collapse_na_summary <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 
  
#> 2: number of object ids for which we do not have any data
obj_id_missing <- which(colormap$objectid %notin% unique(aqli_gadm2_collapse_na_summary$colormap_objectid))

colormap %>%
  filter(objectid %in% obj_id_missing) %>% 
  st_write("./experimentation/missing_obj_ids.shp")

#> 3: number of NAs in population for each colormap object id
aqli_gadm2_collapse_na_summary_pop <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pop = sum(is.na(population)), 
    prop_na = round((total_nas_pop/count_rows)*100, 2)) 

#> Counting the total number of pixels corresponding that are NAs

temp_1998 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.Year.with.2021.data/rasterized/0.008/1998.parquet/part-0.parquet", format = "parquet")

foo <- temp_1998 %>%
   group_by(colormap_objectid) %>%
  collect() %>%
  summarize(n = n())

#> comparing the parquet files for 0.008 resolution's 2013 and 1998 years
parquet_1998_0.008 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/1998.parquet/part-0.parquet")

parquet_1998_0.008_summary <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 

parquet_2013_0.008 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/2013.parquet/part-0.parquet")

parquet_2013_0.008_summary <- parquet_2013_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter(colormap_objectid != "NA") %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 

#> comparing the parquet files (0.008 resolution) to see if filtering out NAs before collecting is faster than doing it after collecting data locally in R. Testing both for 2013 (which has NAs as "NA") and 1998 (which has NA as NA special value)

# 12 minutes
foo <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter((!is.na(colormap_objectid)) & (colormap_objectid != "NA")) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE), 
            lyl_rel_who = round((avg_pm2.5_pollution - who_pm2.5_standard)*aqli_lyl_constant, 2), 
            lyl_rel_who = ifelse(lyl_rel_who < 0, 0, lyl_rel_who)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# 12 minutes
t1 <- Sys.time()
foo1 <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter((!is.na(colormap_objectid)) & (colormap_objectid != "NA")) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t2 <- Sys.time()  

# 1.56 minutes to 3 minutes, wow!!! 
t3 <- Sys.time()
foo2 <- parquet_1998_0.008 %>%
   dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t4 <- Sys.time()

# 4.62 minutes
t5 <- Sys.time()
foo2 <- parquet_1998_0.008 %>%
   dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE), 
            lyl_rel_who = round((avg_pm2.5_pollution - who_pm2.5_standard)*aqli_lyl_constant, 2), 
            lyl_rel_who = ifelse(lyl_rel_who < 0, 0, lyl_rel_who)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t6 <- Sys.time()

# gadm2 level final shapefile test and plot map

map1_global_aqli_color_scale_region_data <- aqli_gadm2_collapse_master_with_geom %>%
  mutate(lyl_aqli_bucket = ifelse((llw2020 >= 0) & (llw2020 < 0.1), "0 - < 0.1 years", NA), 
         lyl_aqli_bucket = ifelse((llw2020 >= 0.1) & (llw2020 < 0.5), "0.1 - < 0.5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 0.5) & (llw2020 < 1), "0.5 - < 1", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 1) & (llw2020 < 2), "1 - < 2", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 2) & (llw2020 < 3), "2 - < 3", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 3) & (llw2020 < 4), "3 - < 4", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 4) & (llw2020 < 5), "4 - < 5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 5) & (llw2020 < 6), "5 - < 6", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 6), ">= 6 years", lyl_aqli_bucket)) %>%
  mutate(order_var = ifelse(lyl_aqli_bucket == "0 - < 0.1 years", 1, NA), 
         order_var = ifelse(lyl_aqli_bucket == "0.1 - < 0.5", 2, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "0.5 - < 1", 3, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "1 - < 2", 4, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "2 - < 3", 5, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "3 - < 4", 6, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "4 - < 5", 7, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "5 - < 6", 8, order_var), 
         order_var = ifelse(lyl_aqli_bucket == ">= 6 years", 9, order_var)) 

# map1
map1_global_aqli_color_scale_region_data %>%
  filter(country == "United States", name1 %notin% c("Alaska")) %>%
  ggplot() +
  geom_sf(mapping = aes(fill = lyl_aqli_bucket), color = "transparent") +
  scale_fill_manual(values = c("0 - < 0.1 years" = "#FFFFFF", "0.1 - < 0.5" = "#FFE6B3", "0.5 - < 1" = "#FFD25D", 
                                "1 - < 2" = "#FFBA00", "2 - < 3" = "#FF9600", "3 - < 4" = "#FF6908", 
                                "4 - < 5" = "#E63D23", "5 - < 6" = "#BD251C", ">= 6 years" = "#8C130E")) +
  ggthemes::theme_map() +
  labs(fill = "Gain in years of Life Expectancy", title = "AQLI 2021 data: Life years lost") +
  theme(legend.position = "bottom", 
        plot.title = element_text(size = 15, hjust = 0.5), 
        legend.justification = c(0.5, 3), 
        plot.background = element_rect(fill = "aliceblue")) +
    guides(fill = guide_legend(nrow = 1)) 

ggsave(filename = "./experimentation/test_aqli_global_2021_map.pdf", plot = map1_global_aqli_color_scale, height = 14, width = 14)

# map2

#-------------------------------------------------------------------------------

# Taking a look at Al Qahirah, Egypt (sanity check suggested by Aaron)

egypt_missing_region_1 <- colormap %>% filter(NAME_0 == "Egypt", NAME_1 == "Al Qahirah", NAME_2 == "Bab ash-Sha'riyah") 

st_coordinates(egypt_missing_region_1)

# Taking a look at 2018 raw net cdf file
pol_2018_0.01_test <- raster::raster("./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.201801-201812.nc")

raster::crs(pol_2018_0.01_test) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84"

pol_2018_0.01_test_df <- raster::as.data.frame(pol_2018_0.01_test)

df_test <- tibble(lat = coordinates(pol_2018_0.01_test)[, 2], 
                  lon = coordinates(pol_2018_0.01_test)[, 1], 
                  pm2.5 = pol_2018_0.01_test_df$Hybrid.PM_2_._5.with.mineral.dust.and.seasalt.removed...mug.m.3.)

df_test1 <- arrow::open_dataset("./experimentation/df_test.parquet/part-0.parquet", format = "parquet")

df_test1_point <- df_test1 %>%
  dplyr::filter(lat >= 30, lat <= 30.07, lon >= 31.1, lon <= 31.3) %>%
  dplyr::collect()


df_test %>% arrow::write_dataset("./experimentation/df_test.parquet", format = "parquet")

# using nc package

pol_2018_0.01_test_nc_fun <- nc_open("./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.201801-201812.nc")


df_test <- tibble(latitude = lat, longitude = lon, pm2.5 = pol_2018_0.01_test_df$Hybrid.PM_2_._5.with.mineral.dust.and.seasalt.removed...mug.m.3.)

#> Sanity checks on the AQLI 2021 processed dataset

#- Comparing population column of color_2020 with the 2021 color file (gadm2)

# 2020 aqli data
color_2020 <- read_csv("C:/Users/Aarsh/Desktop/aqli_2020_data/color_2020.csv")

# 2021 aqli datasets
aqli_gadm2_collapse_master <- read_csv(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_non_geom_Jan192023.csv"))

aqli_gadm0_collapse_from_gadm2 <- read_csv(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_non_geom_Jan192023.csv"))

aqli_gadm1_collapse_from_gadm2 <- read_csv(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_non_geom_Jan192023.csv"))

# joining 2020 gadm2 dataset with 2021 gadm2 dataset and comparing the population column
gadm2_2020_2021_pop_compare <- color_2020 %>% # 46702 rows
  select(country, name_1, name_2, population) %>%
  rename(population_old = population) %>%
  left_join(aqli_gadm2_collapse_master %>% select(country, name_1, name_2, population))

# looking at a subset that contains only those rows in which we have a population number for both 2021 and 2020 data
gadm2_2020_2021_pop_compare_sub <- gadm2_2020_2021_pop_compare %>% # 42679 rows
  filter(!is.na(population_old) & !is.na(population)) 

# calculate pop difference b/w 2021 and 2020 gadm 2 level data
gadm2_2020_2021_pop_compare_sub <- gadm2_2020_2021_pop_compare_sub %>%
  mutate(pop_diff = population - population_old)

# For places, where population has decreased, plotting a distribution of abs(pop_diff). It seems like at max population has decreased by 10^5. Note that there were many border changes and regrouping of regions and shapefile updates. I personally don't think that its unreasonable. But still check
plt <- gadm2_2020_2021_pop_compare_sub %>%
  filter(pop_diff < 0) %>%
  mutate(pop_diff_abs = abs(pop_diff)) %>%
  ggplot() +
  geom_density(mapping = aes(x = log10(pop_diff_abs)), color = "black")

# comparing population distribution of color_2020 with color_2021
plt <- gadm2_2020_2021_pop_compare_sub %>%
  ggplot() +
  geom_histogram(mapping = aes(x = log10(population)), color = "white", fill = "cornflowerblue", alpha = 0.3) + 
  geom_histogram(mapping = aes(x = log10(population_old)), color = "white", fill = "red", alpha = 0.3)

#> check if any place (at gadm2/1/0) has a population of 0, yes: 97 places
pop_0_gadm2_regions <- aqli_gadm2_collapse_master %>%   # 97 regions
  filter(population == 0)

pop_0_gadm1_regions <- aqli_gadm1_collapse_from_gadm2 %>% # 27 regions
  filter(population == 0)

pop_0_gadm0_regions <- aqli_gadm0_collapse_from_gadm2 %>% # 6 regions 
  filter(population == 0)


# look for the above missing gadm2 regions in color_2020 file
color_2020 %>%
  filter(country %in% unique(pop_0_gadm2_regions$country), 
         name_1 %in% unique(pop_0_gadm2_regions$name_1), 
         name_2 %in% unique(pop_0_gadm2_regions$name_2)) %>%
  View()

# compare pm2.5 pollution distribution for 2020 v/s 2021 pollution data

plt <- color_2020 %>%
  filter(country == "Germany") %>%
  ggplot() +
  geom_histogram(mapping = aes(x = pm2020), fill = "cornflowerblue", color = "white", alpha = 0.3) +
  geom_histogram(data = aqli_gadm2_collapse_master %>% filter(country == "Germany"), mapping = aes(x = pm2020), fill = "red", color = "white", alpha = 0.3) 

# in places where there is 0 population at gadm level 2, is there pollution data available in the 2021 file and also in 2020 file. Answer: in 2020 it is available, but in 2021 no, because it probably was available, but went away when population weighting happened.

color_2020 %>%
  filter(country %in% pop_0_gadm2_regions$country, name_1 %in% pop_0_gadm2_regions$name_1, name_2 %in% pop_0_gadm2_regions$name_2) %>%
  View()

#> what country is not available in the collapsed gadm0 file, but is available in colormap
rel_ind <- unique(colormap$NAME_0) %notin% unique(aqli_gadm0_collapse_from_gadm2_with_geom$country)
unique(colormap$NAME_0)[rel_ind]

# the answer is "Svalbard and Jan Mayen", which comes under the list of regions we expect to be missing, because its outside the thresholds for which pollution data is available.

#> check the dimensions of each year's dataset and also check if all of the above sanity checks hold true in each year's dataset. 
 
#> See if there is a way to process the islands data in a special way, so that their population doesn't end up being 0.

#> plot timeseries/heatmap of last 24 years country wise and compare with old data counterpart heatmaps
 
#> trendlines, country wise compare 2020 v/s 2021 data
plt0_us <- color_2020 %>%
  filter(country == "United States") %>%
  mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         mutate(across(starts_with("pm"), ~.x*pop_weights, .names = "{col}_weighted"))) %>%
  summarise(across(ends_with("weighted"), sum)) %>%
  pivot_longer(cols = pm1998_weighted:pm2020_weighted, names_to = "years", 
               values_to = "pop_weighted_avg_pm2.5") %>%
  mutate(years = as.integer(unlist(str_extract(years, "\\d+"))), 
         region = "National Average") %>% 
  select(years, region, pop_weighted_avg_pm2.5) %>%
  ggplot() +
  geom_line(mapping = ggplot2::aes(x = as.integer(years), y = as.double(pop_weighted_avg_pm2.5)), lwd = 1.1, 
            color = "darkred") +
  scale_y_continuous(breaks = seq(0, 20, 5), limits = c(0, 20)) +
  scale_x_continuous(breaks = seq(1998, 2020, 2))  +
  ggthemes::theme_clean() +
  labs(x = "Years", 
       y = expression(paste("Average PM2.5 concentration ( ", mu, "g", "/", m^3, " )")), 
       title = "PM2.5 Pollution trend 1998 to 2020 (United States)") +
  theme(legend.position = "bottom", legend.title = element_blank(), 
        legend.text = element_text(size = 7), 
        axis.title.y = element_text(size = 9), 
        axis.title.x = element_text(size = 9), 
        axis.text.x = element_text(size = 6), 
        axis.text.y = element_text(size = 6)) 
  # geom_text(x = 2002.8, y = 8.7, label = expression(paste("WHO PM2.5 Guideline (last updated: 2021): 5 ", mu, "g","/", m^3, "")))

plt_us <- grid.arrange(plt0_us, plt1_us, nrow = 1)
 
#> heatmap continent wise

# read in continent-country mapping file
country_continent <- read_csv("./experimentation/others/country_continent.csv")

color_2020 <- color_2020 %>%
  left_join(country_continent, by = c("country"))

aqli_gadm2_collapse_master_continent <-  aqli_gadm2_collapse_master %>%
  left_join(country_continent, by = c("country"))


country_wise_pm2.5_average <- aqli_gadm2_collapse_master_continent  %>%
  filter(continent == "Africa") %>%
  group_by(country) %>%
  mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         mutate(across(starts_with("pm"), ~.x*pop_weights, .names = "{col}_weighted"))) %>%
  summarise(across(ends_with("weighted"), sum)) %>%
  pivot_longer(cols = pm1998_weighted:pm2021_weighted, names_to = "years", 
               values_to = "pop_weighted_avg_pm2.5") %>%
  mutate(years = as.integer(unlist(str_extract(years, "\\d+"))), 
         region = "National Average") %>% 
  arrange(pop_weighted_avg_pm2.5) 

heatmap_africa_2021 <- country_wise_pm2.5_average %>%
  filter(country %in% c("Democratic Republic of the Congo", 
                        "Rwanda", 
                        "Burundi", 
                        "Cameroon", 
                        "Uganda", 
                        "Angola", 
                        "Benin", 
                        "Ghana", 
                        "Gabon",
                        "Burkina Faso",
                        "South Africa",
                        "Egypt", 
                        "Libya",
                        "Niger", 
                        "Mauritius")) %>%
  mutate(le_lost = (pop_weighted_avg_pm2.5 - 5)*0.098, 
         le_lost = ifelse(le_lost < 0, 0, le_lost)) %>%
  ggplot() +
  geom_raster(mapping = aes(x = years, y = fct_reorder(country, pop_weighted_avg_pm2.5), fill = pop_weighted_avg_pm2.5)) +
   scale_fill_viridis_b(option = "rocket", direction = -1) +
  scale_x_continuous(breaks = seq(1998, 2021, 1)) +
  labs(x = "Years", y = "", fill = "PM2.5 pollution (g/m)", caption = str_wrap("*This graph displays the life years lost relative to the WHO PM2.5 guideline in all countries of the African continent (from 1998 to 2021)")) +
  ggthemes::theme_hc() +
  theme(axis.text.x = element_text(size = 5), 
        axis.text.y = element_text(size = 7)) 

heatmap_africa_final <- grid.arrange(heatmap_africa_2020, heatmap_africa_2021, nrow = 1)

#> More sanity checks on the llpp columns calculations


```



# tmp 

```{r}

```

