---
title: "v1 of the new workflow: to be shared with VIT by January 15th, 2023"
author: "Aarsh (aarshbatra@uchicagotrust.org/aarshbatra.in@gmail.com)"
date: '2022-10-18'
output: html_document
---

# setup
```{r setup}
knitr::opts_chunk$set(echo = FALSE)
# start time
start_time <- Sys.time()

# load libraries
library(raster)
library(rgdal)
library(dplyr)
library(readr)
library(ncdf4)
library(assertthat)
library(fasterize)
library(sf)
library(SpaDES)
library(foster) # for matching resolution of 2 different rasters
library(DBI) # for connecting R with Postgres
library(RPostgres)
library(ggplot2)
library(RPostgres)
# library(sparklyr)
library(data.table)
library(stringr)

# global variables (check for updates, if any)
who_pm2.5_standard <- 5 # in micrograms per cubic meter, annual average PM2.5 standard
aqli_lyl_constant <- 0.098
india_pm2.5_standard <- 40 # in micrograms per cubic meter
region_pm2.5_standard <- 15 # China

print("Libraries and Global variables loaded in.")

```

#> Using the new workflow, generate gadm level 2 population weighted pollution and life years lost numbers (WHO and National Standard) for a given year's Global data. Standalone, uses its own data files, not the data files from the second chunk up top.

# set paths

```{r set_paths, echo=FALSE}

#> paths and global variables (create the necessary folder structure after reading through the paths section. All paths are relative to your current working directory, but you might need to make a few folders, for e.g. for specific resolution datasets). After that, run this script and everything should run smoothly.----------------------------------------

# pollution
pol_data_location <-"./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/"



# population
pop_data_location <- "./ar.2023.update.using.2021.pol.data/data/input/population/"
pop_data_file_name <- "landscan-global-2021.tif"



#> shapefiles

# general shape file folder location
shp_files_location <- "./ar.2023.update.using.2021.pol.data/data/intermediate/1_population_and_colormap/1_shapefile_aggregate/"

# gadm2 shape file location
colormap_location <- "colormap/colormap.shp"

# hover map shape file location (not using this year)
hovermap_location <- "hover/hover.shp"

# gadm0 shape file location
gadm0_shp_file_location <- "colormap_collapsed_gadm0/aqli_gadm2_collapse_to_gadm0.shp" 

# gadm1 shape file location
gadm1_shp_file_location <- "colormap_collapsed_gadm1/aqli_gadm2_collapse_to_gadm1.shp" 


#> raster resolution of the final data brick (containing a rasterized pollution, population and a rasterized shape file)
raster_res <- 0.008

# data timeline (note)
pol_data_start_year <- 1998
update_year <- 2021

# data levels
gadm0_folder_name <- "gadm_0"
gadm1_folder_name <- "gadm_1"
gadm2_folder_name <- "gadm_2"

# corresponnding aqli report publishing year (this is the year in which "update_year"'s data will be published. Current lag is 2 years).
report_publishing_year <- 2023

# ssd aqli folder high res (0.01 as in December, 2022) location (use ssd for high writing speeds, hdd's suck)

# drive location
ssd_drive <- "D:/"

# data folder name (on ssd)
aqli_data_share_folder_name <- "aqli.2023.report.data.share"

# high res data location
ssd_location_rasterized_data_0.008 <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".data", "/rasterized/", raster_res, "/", sep = "")

# updated national standards file location and file name
national_standards_pm2.5_jan_2023_location <- "./ar.2023.update.using.2021.pol.data/data/input/standards/" 

national_standards_pm2.5_jan_2023_file_name <- "country_annual_average_pm2.5_standards_asInJan2023.csv"

#> collapsed data path: gadm0 level
ssd_location_collapsed_gadm0_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm0_folder_name, "/", sep  = "")

#> collapsed data path: gadm1 level
ssd_location_collapsed_gadm1_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm1_folder_name, "/", sep  = "")

#> collapsed data path: gadm2 level
ssd_location_collapsed_gadm2_data_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm2_folder_name, "/", sep  = "")


#-------------------------------------------------

```

# main pipeline to get yearly gadm2 and high res pollution datasets

```{r get_yearly_pol_datasets, echo=FALSE}

# benchmarking
threshold_0 <- Sys.time() 

# population raw data
population_dataset <- raster::raster(str_c(pop_data_location, pop_data_file_name, sep = ""))

# naming the raw global landscan population data and setting its crs to be the same as the pollution data
raster::crs(population_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(population_dataset) <- "population"


#--- no longer need to crop, as this can be reused, so I wrote it as a tif, and then will simply read it
#- crop population dataset to the pollution dataset
# pop_raw_landscan_crop_pol <- raster::crop(population_dataset, pollution_dataset)

#- write the cropped population dataset
# pop_raw_landscan_crop_pol %>%
#   raster::writeRaster(filename = "./experimentation/pop_raw_landscan_crop_pol.tif", 
#                       format = "GTiff", overwrite = TRUE)
#---

# reading in the pre-cropped population raster, which remains the same for all pollution datasets
pop_raw_landscan_crop_pol <- raster::raster("./experimentation/pop_raw_landscan_crop_pol.tif")


# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

#-- this remains same for all pollution datasets, hence writing it, and will then simply read it
# polygon_cells <- fasterize(colormap, pol_0.01_region_in_landscan_pop_res, field = "objectid", fun = "last")
# writeRaster(polygon_cells,
# 	filename = "./experimentation/colormap_rasterized.tif",
# 	format = "GTiff", overwrite = TRUE)
#---

polygon_cells <- raster::raster("./experimentation/colormap_rasterized.tif")

# making sure that the crs of the rasterized shapefile is the same as the 0.008 population and pollution rasters. 
raster::crs(polygon_cells) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

# benchmarking
threshold_1 <- Sys.time() 

print("All raw datasets read into R.")

#> reading pollution data, one year at a time and then will concatenate all results-----------------------
# Note that the datasets in this pipeline do not include the  geometry column. That can be added in the end after concatenating all yearly datasets into a single final gadm2 dataset. For the gadm0 and gadm1 datasets (which will be derived from the single "final gadm2 dataset"), a similar process will follow.

# pol data list
pol_data_list <- list.files(pol_data_location) %>% sort()

# pollution column names empty vector
pol_col_name_vec <- c()

#> for loop begins----------------------------------------------------------------

print("For loop begins: Processing pollution rasters 1 year at a time")

for (i in 1:length(pol_data_list)){
  
  # benchmarking
  threshold_1.5 <- Sys.time()
  
  print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " begins"))
  
  pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
  
  # for testing purposes
  if(i < 12){
    next
  }
  
  # if(i > 10){
  #   break
  # }
  
  # pollution file name given the current iteration
  cur_pol_file_name <- pol_data_list[i]
  
  # cur pollution file year
  cur_pol_file_year <- stringr::str_extract(str_extract(cur_pol_file_name, "(\\d+)-(\\d+)"), "....")
  
  # read in the pollution raster for a given year
  pollution_dataset <- raster::raster(str_c(pol_data_location, cur_pol_file_name, sep = ""))
  
  # naming the raw global pollution data and setting its crs
raster::crs(pollution_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(pollution_dataset) <- "pm2.5_pollution"


# benchmarking
threshold_2 <- Sys.time() 

# matching the resolution of the cropped population and pollution datasets
pol_0.01_region_in_landscan_pop_res <- foster::matchResolution(pollution_dataset, pop_raw_landscan_crop_pol)

# benchmarking
threshold_3 <- Sys.time() 

print("stacking all layers in a raster brick")

# creating a raster brick using the population and pollution data
region_raster_brick <- pop_raw_landscan_crop_pol %>% 
  raster::addLayer(pol_0.01_region_in_landscan_pop_res) 

# setting the names of the newly created placheolders
names(region_raster_brick) <- c("population", "pm2.5_pollution")

# set the same crs for pollution brick
raster::crs(region_raster_brick) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

print("Population and pollution layers matched")

# benchmarking
threshold_4 <- Sys.time() 

# Now match each population/pollution point to a colormap polygon. To do this, convert
# polygons to raster of same resolution as population raster, with value of each cell equal
# to objectid of polygon that covers its center.
# Fasterize is an ultra-fast version of the rasterize function.

# add rasterized colormap to the raster brick
region_raster_brick <- region_raster_brick %>% 
  raster::addLayer(polygon_cells)

print("added rasterized colormap layer to the brick")

names(region_raster_brick)[length(names(region_raster_brick))] <- "colormap_objectid"

# benchmarking
threshold_5 <- Sys.time() 

# from this point forward, replace all instances of "region_raster_brick_df" with "aqli_raster_brick_df". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_df <- raster::as.data.frame(region_raster_brick)

print("raster brick converted to data frame")

# write rasterized dataframe to ssd, in arrow data format (0.008x0.008 resolution)
aqli_raster_brick_df %>% arrow::write_dataset(str_c(ssd_location_rasterized_data_0.008, cur_pol_file_year, ".parquet"))

# benchmarking
threshold_6 <- Sys.time() 

#-- (Update: no longer needed as we directly coerce region_raster_brick_df to an arrow table). Will only need to write this when implementing high res layer.
#- write region_raster_brick_df to a csv
#--

# from this point forward, replace all instances of "pollution_data_0.01_light" with "aqli_raster_brick_light". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_light <- arrow::as_arrow_table(aqli_raster_brick_df)

# benchmarking
threshold_7 <- Sys.time() 

print("rasterized dataframe coerced to an arrow table")

#-- read raster data using arrow (no longer needed as we already coerced region_raster_brick_df to an arrow table)
# aqli_raster_brick_light <- arrow::open_dataset("./experimentation/pollution_data_0.01_2021.csv", format = "csv")
#-- 


# from this point forward, replace all instances of "pollution_district_wise" with "aqli_gadm2_collapse". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_gadm2_collapse <- aqli_raster_brick_light %>%
  dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# renaming the pollution column such that it includes the year in question
colnames(aqli_gadm2_collapse)[str_detect(colnames(aqli_gadm2_collapse), "avg_pm2.5_pollution")] <- pol_col_name_vec[i]

# writing the gadm2 level dataset to ssd
aqli_gadm2_collapse %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, cur_pol_file_year, "_", gadm2_folder_name, ".csv"))

# benchmarking
threshold_8 <- Sys.time() 



print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " end"))

print("Freeing memory before proceeding to next iteration")

# benchmarking
threshold_9 <- Sys.time()

# collect garbage, free memory
gc()

# benchmarking
threshold_10 <- Sys.time()

# time it took to complete the current iteration

print(str_c("Time taken to complete iteration # ", i, "/", (update_year - pol_data_start_year) + 1, ": ", threshold_10 - threshold_1.5))

}

end_time_main_pipeline <- Sys.time()
time_diff <- end_time_main_pipeline - start_time

print(str_c("Total time taken (main pipeline for loop) with ", i, " years of pollution data processed: ", time_diff))

#> for loop ends-----------------------------------------------------------------------


```

# combine all pollution yearly datasets into a single gadm2 dataset, in the format that has to be shared with VIT. See AQLI data dictionary for more information.

```{r combine_yearly_pol_data, echo=FALSE}

#---------
#> Note: if you have already processed the pollution datasets in the above chunk and just want to run the combining code in this chunk, make sure to uncomment the following code and run it before proceeding to get the colormap and pol_col_name_vec. Also, make sure to run the "setup" and "set path" chunks. If you have already run the above chunks, then you would already have these objects and in that case there is no need to run the below commented code.

# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

# generate the pol_col_name_vec
pol_col_name_vec <- c()
for(i in 1:24){
     pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
 }

#-----------

# benchmarking
threshold_11 <- Sys.time() 

#> Output in VIT data sharing format

# read in all pollution data and name_0, name_1, name_2 columns from colormap (joined in the first iteration of the loop below) into a single dataset, with just pollution columns. I am in the process of updating national standards, so for now we have a placeholder national standards column, which is just set to 10 micrograms per cubic meter for all regions. The updated national standards column will be added alongside the life years lost columns after the below for loop.

# list of files in the collapsed folder
collapsed_gadm2_files_vec <- list.files(str_c(ssd_location_collapsed_gadm2_data_path)) %>% 
  sort() 

# indices of the relevant files (from the above vector) needed for combining.
collapsed_gadm2_files_vec_rel_ind <- str_detect(collapsed_gadm2_files_vec, ".csv")

# keeping only relevant files that will be combined below
collapsed_gadm2_files_vec_rel_files <- collapsed_gadm2_files_vec[collapsed_gadm2_files_vec_rel_ind]

for(i in 1:length(collapsed_gadm2_files_vec_rel_files)){
  if(i == 1){
    temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))
    
    aqli_gadm2_collapse_master <- temp_gadm2 %>%
      dplyr::left_join(colormap, by = c("objectid_gadm2" = "objectid")) %>%
      dplyr::mutate(whostandard = 5, 
             natstandard = 10) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, NAME_0, NAME_1, NAME_2, total_population,
             whostandard, natstandard,
             pol_col_name_vec[i]) %>%
      dplyr::rename(country = NAME_0,
             name_1 = NAME_1, 
             name_2 = NAME_2,
             population = total_population)
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
    
  } else{
     temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))
    
    aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>% 
      dplyr::left_join(temp_gadm2, by  = "objectid_gadm2") %>%
      dplyr::select(-c(total_population)) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, pol_col_name_vec[1:(i-1)],
             pol_col_name_vec[i])
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
  
  }
}

# benchmarking
threshold_12 <- Sys.time() 

#> add in the updated national standards column (last updated: January 18, 2023 by Aarsh) and bring it in appropriate "join ready" format (to be joined with the combined pollution data before adding life years lost columns)

# reading in the national standards file
national_standards_pm2.5 <- readr::read_csv(stringr::str_c(national_standards_pm2.5_jan_2023_location, national_standards_pm2.5_jan_2023_file_name, sep = ""))

# keeping and renaming relevant columns
national_standards_pm2.5 <- national_standards_pm2.5 %>%
  dplyr::select(country, natstandard_pm2.5_new_2023_report_micr_grm_cubic_meter_op1) %>%
  dplyr::rename(natstandard_updated = natstandard_pm2.5_new_2023_report_micr_grm_cubic_meter_op1)

# replacing the national standards column of the aqli_gadm_collapse_master file (output of the above for loop) with the updated national standards column
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::left_join(national_standards_pm2.5, by = "country") %>%
  dplyr::select(-c(natstandard)) %>%
  dplyr::select(objectid_gadm2:whostandard, natstandard_updated, dplyr::everything()) %>%
  dplyr::rename(natstandard = natstandard_updated)

# replace natstandard == "No national standard", with natstandard == 0, and then changing its data type to "numeric"
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(natstandard = ifelse(natstandard == "No national standard", 0, natstandard))

aqli_gadm2_collapse_master$natstandard <- as.numeric(aqli_gadm2_collapse_master$natstandard)
  
# adding in the life years lost columns and doing some basic cleaning of column names to bring it into the format that we want, as per the AQLI data dictionary

aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, everything()) %>%
    dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))

# benchmarking
threshold_13 <- Sys.time() 

# write the above (non-geometry version) of gadm2 master file to the collapsed/gadm2 folder

aqli_gadm2_collapse_master %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_non_geom_Jan192023.csv"))

#> add in the geometry column by left joining the above with a color file and then write that to the collapsed/gadm2 folder

# joining the gadm2 collapse master file with colormap (using objectid as the joining column), to get the geometry column
aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master %>%
  dplyr::left_join(colormap %>% rename(objectid_gadm2 = objectid), by = "objectid_gadm2") %>%
  dplyr::rename(iso_alpha3 = iso_alpha3.x) %>%
  dplyr::select(-c(iso_alpha3.y, NAME_0, NAME_1, NAME_2))

# creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm2 collapse master dataset, to avoid any confusions.

new_col_names_gadm2_collapse_master_geom_ver <- colnames(aqli_gadm2_collapse_master_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm2", "objidgadm2"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()

# assigning new colnames to the geom version of the gadm2 collapse master file
colnames(aqli_gadm2_collapse_master_with_geom) <- new_col_names_gadm2_collapse_master_geom_ver

# making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file
aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master_with_geom %>%
  sf::st_as_sf() 

aqli_gadm2_collapse_master_with_geom %>%
  sf::write_sf(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_with_geom_Jan192023.shp"))

#-----------


#> collapse the gadm2 file to gadm0 (country) level and write both geom and non-geom versions to their respective folders

# finalizing the gadm0 level dataset
aqli_gadm0_collapse_from_gadm2 <- aqli_gadm2_collapse_master %>%
  dplyr::group_by(country) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE)) %>%
  dplyr::mutate(across(dplyr::starts_with("pm"), ~(.x*pop_weights), .names = "{col}_pop_weighted")) %>%
  dplyr::summarise(across(dplyr::contains("pop_weighted"), ~(round(sum(.x, na.rm = TRUE), 2)), .names = "avg_{col}"),
                   total_population = sum(population, na.rm = TRUE), objectid_gadm2 = objectid_gadm2[1], iso_alpha3 = iso_alpha3[1], whostandard = whostandard[1], natstandard = natstandard[1]) %>%
  select(objectid_gadm2, iso_alpha3, country, total_population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::mutate(objectid_gadm2 = dplyr::row_number()) %>%
  dplyr::rename(population = total_population, 
                objectid_gadm0 = objectid_gadm2) %>%
  dplyr::rename_with(~str_replace_all(.x, "(_pop_weighted)|(avg_)", ""), dplyr::contains("_pop_weighted")) %>%
    dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm0, iso_alpha3, country, population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))
  

# write the above (non-geometry version) of gadm0 master file to the collapsed/gadm0 folder
aqli_gadm0_collapse_from_gadm2 %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_non_geom_Jan192023.csv"))


# read in the gadm0 shape file (which is collapsed from the colormap) and get it ready for joining with the above summary table

gadm0_shp_file <- sf::st_read(str_c(shp_files_location, gadm0_shp_file_location, sep = ""))

gadm0_shp_file_subset <- gadm0_shp_file %>%
  dplyr::select(-c(NAME_1, NAME_2, objectid, iso_alpha3))

# add in the geometry column by left joining the above with a country level shape file and then write that to the collapsed/gadm0 folder

# joining the gadm0 collapse master file with the country level shapefile (using "country/NAME_0" as the joining column), to get the geometry column

aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2 %>%
  dplyr::left_join(gadm0_shp_file_subset, by = c("country" = "NAME_0")) %>%
  sf::st_as_sf()

# (tbd): creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm0 collapse master dataset, to avoid any confusions.

new_col_names_gadm0_collapse_from_gadm2 <- colnames(aqli_gadm0_collapse_from_gadm2_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm0", "objidgadm0"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()


# (tbd): assigning new colnames to the geom version of the gadm0 collapse master file
colnames(aqli_gadm0_collapse_from_gadm2_with_geom) <- new_col_names_gadm0_collapse_from_gadm2

# (tbd): making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file

aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2_with_geom %>%
  sf::st_as_sf()

aqli_gadm0_collapse_from_gadm2_with_geom %>%
  sf::st_write(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_with_geom_Jan192023_.shp"), delete_layer = TRUE)

#-----------

#>  collapse the gadm2 file to gadm1 (state/province) level and write both geom and non-geom versions to their respective folders

# finalizing the gadm1 level dataset
aqli_gadm1_collapse_from_gadm2 <- aqli_gadm2_collapse_master %>%
  dplyr::group_by(country, name_1) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE)) %>%
  dplyr::mutate(across(dplyr::starts_with("pm"), ~(.x*pop_weights), .names = "{col}_pop_weighted")) %>%
  dplyr::summarise(across(dplyr::contains("pop_weighted"), ~(round(sum(.x, na.rm = TRUE), 2)), .names = "avg_{col}"),
                   total_population = sum(population, na.rm = TRUE), objectid_gadm2 = objectid_gadm2[1], iso_alpha3 = iso_alpha3[1], whostandard = whostandard[1], natstandard = natstandard[1]) %>%
  select(objectid_gadm2, iso_alpha3, country, name_1, total_population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::mutate(objectid_gadm2 = dplyr::row_number()) %>%
  dplyr::rename(population = total_population, 
                objectid_gadm1 = objectid_gadm2) %>%
  dplyr::rename_with(~str_replace_all(.x, "(_pop_weighted)|(avg_)", ""), dplyr::contains("_pop_weighted")) %>%
    dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who_{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat_{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(natstandard == 0, NA, .x))) %>%
  dplyr::select(objectid_gadm1, iso_alpha3, country, name_1, population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 1)), .names = "{col}"))
  

# write the above (non-geometry version) of gadm1 master file to the collapsed/gadm1 folder
aqli_gadm1_collapse_from_gadm2 %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_non_geom_Jan192023.csv"))

# read in the gadm1 shape file (which is collapsed from the colormap) and get it ready for joining with the above summary table

gadm1_shp_file <- sf::st_read(str_c(shp_files_location, gadm1_shp_file_location, sep = ""))


gadm1_shp_file_subset <- gadm1_shp_file %>%
  dplyr::select(-c(NAME_2, objectid, iso_alpha3))

# (tbd) add in the geometry column by left joining the above with a state level shape file and then write that to the collapsed/gadm1 folder

# joining the gadm1 collapse master file with the state level shapefile (using " " as the joining column), to get the geometry column

aqli_gadm1_collapse_from_gadm2_with_geom <- aqli_gadm1_collapse_from_gadm2 %>%
  dplyr::left_join(gadm1_shp_file_subset, by = c("country" = "NAME_0", "name_1" = "NAME_1")) %>%
  sf::st_as_sf()


# (tbd): creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm1 collapse master dataset, to avoid any confusions.

new_col_names_gadm1_collapse_from_gadm2 <- colnames(aqli_gadm1_collapse_from_gadm2_with_geom) %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(col_names_shortened = str_replace(value, "llpp", "ll"), 
         col_names_shortened = str_replace(col_names_shortened, "_who_", "w"), 
         col_names_shortened = str_replace(col_names_shortened, "_nat_", "n"), 
         col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
         col_names_shortened = str_replace(col_names_shortened, "_", ""), 
         col_names_shortened = str_replace(col_names_shortened, "objectidgadm1", "objidgadm1"), 
         col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
  dplyr::select(col_names_shortened) %>%
  unlist() %>%
  as.vector()

# (tbd): assigning new colnames to the geom version of the gadm1 collapse master file
colnames(aqli_gadm1_collapse_from_gadm2_with_geom) <- new_col_names_gadm1_collapse_from_gadm2

# (tbd): making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file

aqli_gadm1_collapse_from_gadm2_with_geom <- aqli_gadm1_collapse_from_gadm2_with_geom %>%
  sf::st_as_sf()

aqli_gadm1_collapse_from_gadm2_with_geom %>%
  sf::st_write(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_with_geom_Jan192023.shp"), delete_layer = TRUE)

#-----------
end_time_vit_data_pipeline <- Sys.time()
print(str_c("Total time taken: ", end_time_vit_data_pipeline - start_time,  " minutes?"))
```


# Sanity checks area (deactivated for now)
```{r sanity_checks, eval=FALSE, include=FALSE}
#> sanity checks time-------------------------------------------------------------

#> 1: number of NA's in pollution for each colormap object id
foo <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/1999.parquet/part-0.parquet")

# reading in the raw pollution raster for 1998
pol_raw_1998_tmp <- raster::raster(str_c(pol_data_location, "V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.199801-199812.nc"))

pol_raw_2021_tmp <- raster::raster(str_c(pol_data_location, "V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.202101-202112.nc"))

val_pol_raw_1998_tmp <- values(pol_raw_1998_tmp)
val_pol_raw_2021_tmp <- values(pol_raw_2021_tmp)


aqli_gadm2_collapse_na_summary <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 
  
#> 2: number of object ids for which we do not have any data
obj_id_missing <- which(colormap$objectid %notin% unique(aqli_gadm2_collapse_na_summary$colormap_objectid))

colormap %>%
  filter(objectid %in% obj_id_missing) %>% 
  st_write("./experimentation/missing_obj_ids.shp")

#> 3: number of NAs in population for each colormap object id
aqli_gadm2_collapse_na_summary_pop <- foo %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pop = sum(is.na(population)), 
    prop_na = round((total_nas_pop/count_rows)*100, 2)) 

#> Counting the total number of pixels corresponding that are NAs

temp_1998 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.Year.with.2021.data/rasterized/0.008/1998.parquet/part-0.parquet", format = "parquet")

foo <- temp_1998 %>%
   group_by(colormap_objectid) %>%
  collect() %>%
  summarize(n = n())

#> comparing the parquet files for 0.008 resolution's 2013 and 1998 years
parquet_1998_0.008 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/1998.parquet/part-0.parquet")

parquet_1998_0.008_summary <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 

parquet_2013_0.008 <- arrow::open_dataset("D:/aqli.2023.report.data.share/2023.publish.year.with.2021.data/rasterized/0.008/2013.parquet/part-0.parquet")

parquet_2013_0.008_summary <- parquet_2013_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter(colormap_objectid != "NA") %>%
  dplyr::summarise(count_rows = n(),
    total_nas_pol = sum(is.na(pm2.5_pollution)), 
    prop_na = round((total_nas_pol/count_rows)*100, 2)) 

#> comparing the parquet files (0.008 resolution) to see if filtering out NAs before collecting is faster than doing it after collecting data locally in R. Testing both for 2013 (which has NAs as "NA") and 1998 (which has NA as NA special value)

# 12 minutes
foo <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter((!is.na(colormap_objectid)) & (colormap_objectid != "NA")) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE), 
            lyl_rel_who = round((avg_pm2.5_pollution - who_pm2.5_standard)*aqli_lyl_constant, 2), 
            lyl_rel_who = ifelse(lyl_rel_who < 0, 0, lyl_rel_who)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# 12 minutes
t1 <- Sys.time()
foo1 <- parquet_1998_0.008 %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::filter((!is.na(colormap_objectid)) & (colormap_objectid != "NA")) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t2 <- Sys.time()  

# 1.56 minutes to 3 minutes, wow!!! 
t3 <- Sys.time()
foo2 <- parquet_1998_0.008 %>%
   dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
    dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t4 <- Sys.time()

# 4.62 minutes
t5 <- Sys.time()
foo2 <- parquet_1998_0.008 %>%
   dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE), 
            lyl_rel_who = round((avg_pm2.5_pollution - who_pm2.5_standard)*aqli_lyl_constant, 2), 
            lyl_rel_who = ifelse(lyl_rel_who < 0, 0, lyl_rel_who)) %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)
t6 <- Sys.time()

# gadm2 level final shapefile test and plot map

map1_global_aqli_color_scale_region_data <- aqli_gadm2_collapse_master_with_geom %>%
  mutate(lyl_aqli_bucket = ifelse((llw2020 >= 0) & (llw2020 < 0.1), "0 - < 0.1 years", NA), 
         lyl_aqli_bucket = ifelse((llw2020 >= 0.1) & (llw2020 < 0.5), "0.1 - < 0.5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 0.5) & (llw2020 < 1), "0.5 - < 1", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 1) & (llw2020 < 2), "1 - < 2", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 2) & (llw2020 < 3), "2 - < 3", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 3) & (llw2020 < 4), "3 - < 4", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 4) & (llw2020 < 5), "4 - < 5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 5) & (llw2020 < 6), "5 - < 6", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llw2020 >= 6), ">= 6 years", lyl_aqli_bucket)) %>%
  mutate(order_var = ifelse(lyl_aqli_bucket == "0 - < 0.1 years", 1, NA), 
         order_var = ifelse(lyl_aqli_bucket == "0.1 - < 0.5", 2, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "0.5 - < 1", 3, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "1 - < 2", 4, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "2 - < 3", 5, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "3 - < 4", 6, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "4 - < 5", 7, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "5 - < 6", 8, order_var), 
         order_var = ifelse(lyl_aqli_bucket == ">= 6 years", 9, order_var)) 

# map1
map1_global_aqli_color_scale_region_data %>%
  filter(country == "United States", name1 %notin% c("Alaska")) %>%
  ggplot() +
  geom_sf(mapping = aes(fill = lyl_aqli_bucket), color = "transparent") +
  scale_fill_manual(values = c("0 - < 0.1 years" = "#FFFFFF", "0.1 - < 0.5" = "#FFE6B3", "0.5 - < 1" = "#FFD25D", 
                                "1 - < 2" = "#FFBA00", "2 - < 3" = "#FF9600", "3 - < 4" = "#FF6908", 
                                "4 - < 5" = "#E63D23", "5 - < 6" = "#BD251C", ">= 6 years" = "#8C130E")) +
  ggthemes::theme_map() +
  labs(fill = "Gain in years of Life Expectancy", title = "AQLI 2021 data: Life years lost") +
  theme(legend.position = "bottom", 
        plot.title = element_text(size = 15, hjust = 0.5), 
        legend.justification = c(0.5, 3), 
        plot.background = element_rect(fill = "aliceblue")) +
    guides(fill = guide_legend(nrow = 1)) 

ggsave(filename = "./experimentation/test_aqli_global_2021_map.pdf", plot = map1_global_aqli_color_scale, height = 14, width = 14)

# map2

#-------------------------------------------------------------------------------

# Taking a look at Al Qahirah, Egypt (sanity check suggested by Aaron)

egypt_missing_region_1 <- colormap %>% filter(NAME_0 == "Egypt", NAME_1 == "Al Qahirah", NAME_2 == "Bab ash-Sha'riyah") 

st_coordinates(egypt_missing_region_1)

# Taking a look at 2018 raw net cdf file
pol_2018_0.01_test <- raster::raster("./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.201801-201812.nc")

raster::crs(pol_2018_0.01_test) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84"

pol_2018_0.01_test_df <- raster::as.data.frame(pol_2018_0.01_test)

df_test <- tibble(lat = coordinates(pol_2018_0.01_test)[, 2], 
                  lon = coordinates(pol_2018_0.01_test)[, 1], 
                  pm2.5 = pol_2018_0.01_test_df$Hybrid.PM_2_._5.with.mineral.dust.and.seasalt.removed...mug.m.3.)

df_test1 <- arrow::open_dataset("./experimentation/df_test.parquet/part-0.parquet", format = "parquet")

df_test1_point <- df_test1 %>%
  dplyr::filter(lat >= 30, lat <= 30.07, lon >= 31.1, lon <= 31.3) %>%
  dplyr::collect()


df_test %>% arrow::write_dataset("./experimentation/df_test.parquet", format = "parquet")

# using nc package

pol_2018_0.01_test_nc_fun <- nc_open("./ar.2023.update.using.2021.pol.data/data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/V5GL03.HybridPM25-NoDust-NoSeaSalt.Global.201801-201812.nc")


df_test <- tibble(latitude = lat, longitude = lon, pm2.5 = pol_2018_0.01_test_df$Hybrid.PM_2_._5.with.mineral.dust.and.seasalt.removed...mug.m.3.)

#> Sanity checks on the AQLI 2021 processed dataset

#- Comparing population column of color_2020 with the 2021 color file (gadm2)

# 2020 aqli data
color_2020 <- read_csv("C:/Users/Aarsh/Desktop/aqli_2020_data/color_2020.csv")

# 2021 aqli datasets
aqli_gadm2_collapse_master <- read_csv(str_c(ssd_location_collapsed_gadm2_data_path, "master_global_allyears_gadm2_non_geom_Jan192023.csv"))

aqli_gadm0_collapse_from_gadm2 <- read_csv(str_c(ssd_location_collapsed_gadm0_data_path, "master_global_allyears_gadm0_non_geom_Jan192023.csv"))

aqli_gadm1_collapse_from_gadm2 <- read_csv(str_c(ssd_location_collapsed_gadm1_data_path, "master_global_allyears_gadm1_non_geom_Jan192023.csv"))

# joining 2020 gadm2 dataset with 2021 gadm2 dataset and comparing the population column
gadm2_2020_2021_pop_compare <- color_2020 %>% # 46702 rows
  select(country, name_1, name_2, population) %>%
  rename(population_old = population) %>%
  left_join(aqli_gadm2_collapse_master %>% select(country, name_1, name_2, population))

# looking at a subset that contains only those rows in which we have a population number for both 2021 and 2020 data
gadm2_2020_2021_pop_compare_sub <- gadm2_2020_2021_pop_compare %>% # 42679 rows
  filter(!is.na(population_old) & !is.na(population)) 

# calculate pop difference b/w 2021 and 2020 gadm 2 level data
gadm2_2020_2021_pop_compare_sub <- gadm2_2020_2021_pop_compare_sub %>%
  mutate(pop_diff = population - population_old)

# For places, where population has decreased, plotting a distribution of abs(pop_diff). It seems like at max population has decreased by 10^5. Note that there were many border changes and regrouping of regions and shapefile updates. I personally don't think that its unreasonable. But still check
plt <- gadm2_2020_2021_pop_compare_sub %>%
  filter(pop_diff < 0) %>%
  mutate(pop_diff_abs = abs(pop_diff)) %>%
  ggplot() +
  geom_density(mapping = aes(x = log10(pop_diff_abs)), color = "black")

# comparing population distribution of color_2020 with color_2021
plt <- gadm2_2020_2021_pop_compare_sub %>%
  ggplot() +
  geom_histogram(mapping = aes(x = log10(population)), color = "white", fill = "cornflowerblue", alpha = 0.3) + 
  geom_histogram(mapping = aes(x = log10(population_old)), color = "white", fill = "red", alpha = 0.3)

#> check if any place (at gadm2/1/0) has a population of 0, yes: 97 places
pop_0_gadm2_regions <- aqli_gadm2_collapse_master %>%   # 97 regions
  filter(population == 0)

pop_0_gadm1_regions <- aqli_gadm1_collapse_from_gadm2 %>% # 27 regions
  filter(population == 0)

pop_0_gadm0_regions <- aqli_gadm0_collapse_from_gadm2 %>% # 6 regions 
  filter(population == 0)


# look for the above missing gadm2 regions in color_2020 file
color_2020 %>%
  filter(country %in% unique(pop_0_gadm2_regions$country), 
         name_1 %in% unique(pop_0_gadm2_regions$name_1), 
         name_2 %in% unique(pop_0_gadm2_regions$name_2)) %>%
  View()

# compare pm2.5 pollution distribution for 2020 v/s 2021 pollution data

plt <- color_2020 %>%
  filter(country == "Germany") %>%
  ggplot() +
  geom_histogram(mapping = aes(x = pm2020), fill = "cornflowerblue", color = "white", alpha = 0.3) +
  geom_histogram(data = aqli_gadm2_collapse_master %>% filter(country == "Germany"), mapping = aes(x = pm2020), fill = "red", color = "white", alpha = 0.3) 

# in places where there is 0 population at gadm level 2, is there pollution data available in the 2021 file and also in 2020 file. Answer: in 2020 it is available, but in 2021 no, because it probably was available, but went away when population weighting happened.

color_2020 %>%
  filter(country %in% pop_0_gadm2_regions$country, name_1 %in% pop_0_gadm2_regions$name_1, name_2 %in% pop_0_gadm2_regions$name_2) %>%
  View()

#> what country is not available in the collapsed gadm0 file, but is available in colormap
rel_ind <- unique(colormap$NAME_0) %notin% unique(aqli_gadm0_collapse_from_gadm2_with_geom$country)
unique(colormap$NAME_0)[rel_ind]

# the answer is "Svalbard and Jan Mayen", which comes under the list of regions we expect to be missing, because its outside the thresholds for which pollution data is available.

#> check the dimensions of each year's dataset and also check if all of the above sanity checks hold true in each year's dataset. 
 
#> See if there is a way to process the islands data in a special way, so that their population doesn't end up being 0.

#> plot timeseries/heatmap of last 24 years country wise and compare with old data counterpart heatmaps
 
#> trendlines, country wise compare 2020 v/s 2021 data
plt0_us <- color_2020 %>%
  filter(country == "United States") %>%
  mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         mutate(across(starts_with("pm"), ~.x*pop_weights, .names = "{col}_weighted"))) %>%
  summarise(across(ends_with("weighted"), sum)) %>%
  pivot_longer(cols = pm1998_weighted:pm2020_weighted, names_to = "years", 
               values_to = "pop_weighted_avg_pm2.5") %>%
  mutate(years = as.integer(unlist(str_extract(years, "\\d+"))), 
         region = "National Average") %>% 
  select(years, region, pop_weighted_avg_pm2.5) %>%
  ggplot() +
  geom_line(mapping = ggplot2::aes(x = as.integer(years), y = as.double(pop_weighted_avg_pm2.5)), lwd = 1.1, 
            color = "darkred") +
  scale_y_continuous(breaks = seq(0, 20, 5), limits = c(0, 20)) +
  scale_x_continuous(breaks = seq(1998, 2020, 2))  +
  ggthemes::theme_clean() +
  labs(x = "Years", 
       y = expression(paste("Average PM2.5 concentration ( ", mu, "g", "/", m^3, " )")), 
       title = "PM2.5 Pollution trend 1998 to 2020 (United States)") +
  theme(legend.position = "bottom", legend.title = element_blank(), 
        legend.text = element_text(size = 7), 
        axis.title.y = element_text(size = 9), 
        axis.title.x = element_text(size = 9), 
        axis.text.x = element_text(size = 6), 
        axis.text.y = element_text(size = 6)) 
  # geom_text(x = 2002.8, y = 8.7, label = expression(paste("WHO PM2.5 Guideline (last updated: 2021): 5 ", mu, "g","/", m^3, "")))

plt_us <- grid.arrange(plt0_us, plt1_us, nrow = 1)
 
#> heatmap continent wise

# read in continent-country mapping file
country_continent <- read_csv("./experimentation/others/country_continent.csv")

color_2020 <- color_2020 %>%
  left_join(country_continent, by = c("country"))

aqli_gadm2_collapse_master_continent <-  aqli_gadm2_collapse_master %>%
  left_join(country_continent, by = c("country"))


country_wise_pm2.5_average <- aqli_gadm2_collapse_master_continent  %>%
  filter(continent == "Africa") %>%
  group_by(country) %>%
  mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         mutate(across(starts_with("pm"), ~.x*pop_weights, .names = "{col}_weighted"))) %>%
  summarise(across(ends_with("weighted"), sum)) %>%
  pivot_longer(cols = pm1998_weighted:pm2021_weighted, names_to = "years", 
               values_to = "pop_weighted_avg_pm2.5") %>%
  mutate(years = as.integer(unlist(str_extract(years, "\\d+"))), 
         region = "National Average") %>% 
  arrange(pop_weighted_avg_pm2.5) 

heatmap_africa_2021 <- country_wise_pm2.5_average %>%
  filter(country %in% c("Democratic Republic of the Congo", 
                        "Rwanda", 
                        "Burundi", 
                        "Cameroon", 
                        "Uganda", 
                        "Angola", 
                        "Benin", 
                        "Ghana", 
                        "Gabon",
                        "Burkina Faso",
                        "South Africa",
                        "Egypt", 
                        "Libya",
                        "Niger", 
                        "Mauritius")) %>%
  mutate(le_lost = (pop_weighted_avg_pm2.5 - 5)*0.098, 
         le_lost = ifelse(le_lost < 0, 0, le_lost)) %>%
  ggplot() +
  geom_raster(mapping = aes(x = years, y = fct_reorder(country, pop_weighted_avg_pm2.5), fill = pop_weighted_avg_pm2.5)) +
   scale_fill_viridis_b(option = "rocket", direction = -1) +
  scale_x_continuous(breaks = seq(1998, 2021, 1)) +
  labs(x = "Years", y = "", fill = "PM2.5 pollution (µg/m³)", caption = str_wrap("*This graph displays the life years lost relative to the WHO PM2.5 guideline in all countries of the African continent (from 1998 to 2021)")) +
  ggthemes::theme_hc() +
  theme(axis.text.x = element_text(size = 5), 
        axis.text.y = element_text(size = 7)) 

heatmap_africa_final <- grid.arrange(heatmap_africa_2020, heatmap_africa_2021, nrow = 1)

#> More sanity checks on the llpp columns calculations


#> Checking the areas of the missing shapefile polygons and checking if the area is smaller than a 0.008 degree lat x 0.008 degree long pixel.

# area calculations example
c_russia <- colormap %>% filter(NAME_0 == "Russia")
c_russia <- st_transform(c_russia, 4326)
c_russia <- c_russia %>% st_make_valid()
c_russia$area <- st_area(c_russia)/(10^6)

# read in 2021 processed aqli gadm2 data
aqli_2021_gadm2_test <- read_csv("./ar.2023.update.using.2021.pol.data/data/output_vit/gadm2_aqli2021.csv")

# get indices for missing areas
missing_ids_index <- colormap$objectid %notin% aqli_2021_gadm2_test$objectid_gadm2

# missing areas
missing_areas <- colormap[missing_ids_index, ]

# calculating areas for missing areas and filetering areas whose area is less than or equal to  a threshold
missing_areas <- st_transform(missing_areas, 4326)
missing_areas <- st_make_valid(missing_areas)

missing_areas$area <- as.numeric(st_area(missing_areas)/(10^6))

missing_areas_drop_geom <- missing_areas %>%
  mutate(area_sans_units = as.numeric(area)) 
  st_drop_geometry()

missing_areas_drop_geom %>%
  slice_max(area_sans_units, n = 10) 

# regions that we expect to be missing and filtering them out of the above list
missing_regions_exp_to_be_missing <- readxl::read_xlsx("C:/Users/Aarsh/Desktop/others/part-1_expectToBeMissing.xlsx")

missing_regions_dont_expect_missing <- missing_areas_drop_geom %>%
  filter(NAME_0 %notin% missing_regions_exp_to_be_missing$NAME_0 & 
        NAME_1 %notin% missing_regions_exp_to_be_missing$NAME_1 &
        NAME_2 %notin% missing_regions_exp_to_be_missing$NAME_2)

#> separately fixing nat standard NA issue and 0 population regions issue and exporting files to be sent out to ViT (later on add these steps to the main codebase, if it makes sense)

# read in files: gadm2 and wherever natstandard = 0, replace it with NA
aqli_2021_data_gadm2 <- readr::read_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/gadm2_aqli2021.csv")  

aqli_2021_data_gadm2 <- aqli_2021_data_gadm2 %>%
  mutate(natstandard = ifelse(natstandard == 0, NA, natstandard)) 


# read in files: gadm1 and wherever natstandard = 0, replace it with NA
aqli_2021_data_gadm1 <- readr::read_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/gadm1_aqli2021.csv")

aqli_2021_data_gadm1 <- aqli_2021_data_gadm1 %>%
  mutate(natstandard = ifelse(natstandard == 0, NA, natstandard)) 

# read in files: gadm0 and wherever natstandard = 0, replace it with NA
aqli_2021_data_gadm0 <- readr::read_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/gadm0_aqli2021.csv")

aqli_2021_data_gadm0 <- aqli_2021_data_gadm0 %>%
  mutate(natstandard = ifelse(natstandard == 0, NA, natstandard)) 

#### prepping gadm2 ViT version

# gadm2 replace all 0's with NAs in rows where population = 0
gadm2_missing_pop <- aqli_2021_data_gadm2 %>%
  filter(population == 0)

# replace all 0's with NAs
gadm2_missing_pop[gadm2_missing_pop == 0] <- NA

# place this back in the original gadm2 data frame
aqli_2021_data_gadm2_vit_ready <- aqli_2021_data_gadm2 %>% 
  dplyr::filter(population != 0) %>%
  dplyr::bind_rows(gadm2_missing_pop) %>%
  dplyr::arrange(objectid_gadm2)

#### prepping gadm1 ViT version

# gadm1 replace all 0's with NAs in rows where population = 0
gadm1_missing_pop <- aqli_2021_data_gadm1 %>%
  filter(population == 0)

# replace all 0's with NAs
gadm1_missing_pop[gadm1_missing_pop == 0] <- NA

# place this back in the original gadm2 data frame
aqli_2021_data_gadm1_vit_ready <- aqli_2021_data_gadm1 %>% 
  dplyr::filter(population != 0) %>%
  dplyr::bind_rows(gadm1_missing_pop) %>%
  dplyr::arrange(objectid_gadm1)


#### prepping gadm0 ViT version

# gadm0 replace all 0's with NAs in rows where population = 0
gadm0_missing_pop <- aqli_2021_data_gadm0 %>%
  filter(population == 0)

# replace all 0's with NAs
gadm0_missing_pop[gadm0_missing_pop == 0] <- NA

# place this back in the original gadm2 data frame
aqli_2021_data_gadm0_vit_ready <- aqli_2021_data_gadm0 %>% 
  dplyr::filter(population != 0) %>%
  dplyr::bind_rows(gadm0_missing_pop) %>%
  dplyr::arrange(objectid_gadm0)

# export final datasets
aqli_2021_data_gadm2_vit_ready %>%
  write_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/main_4_files/gadm2_aqli2021_vit.csv")

aqli_2021_data_gadm1_vit_ready %>%
  write_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/main_4_files/gadm1_aqli2021_vit.csv")

aqli_2021_data_gadm0_vit_ready %>%
  write_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/main_4_files/gadm0_aqli2021_vit.csv")

# read in the AQLI 2021 dataset at gadm level 2
aqli_2021_data_gadm2 <- read_csv("./experimentation/AQLI_2021_data_share_internally_1/AQLI_2021_data_share_internally/vit_files_share_feb2023/gadm2_aqli2021_vit.csv")

# one big picture sanity check (plotting the gadm2 vit ready file)

aqli_2021_data_gadm2_vit_ready_shp <- aqli_2021_data_gadm2 %>%
  dplyr::left_join(colormap %>% rename(objectid_gadm2 = objectid), by = "objectid_gadm2") %>%
  dplyr::rename(iso_alpha3 = iso_alpha3.x) %>%
  dplyr::select(-c(iso_alpha3.y, NAME_0, NAME_1, NAME_2))

# adding lyl buckets to create a lyl map
aqli_2021_data_gadm2_vit_ready_shp_final <- aqli_2021_data_gadm2_vit_ready_shp %>%
  mutate(lyl_aqli_bucket = ifelse((llpp_who_2021 >= 0) & (llpp_who_2021 < 0.1), "0 - < 0.1 years", NA), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 0.1) & (llpp_who_2021 < 0.5), "0.1 - < 0.5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 0.5) & (llpp_who_2021 < 1), "0.5 - < 1", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 1) & (llpp_who_2021 < 2), "1 - < 2", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 2) & (llpp_who_2021 < 3), "2 - < 3", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 3) & (llpp_who_2021 < 4), "3 - < 4", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 4) & (llpp_who_2021 < 5), "4 - < 5", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 5) & (llpp_who_2021 < 6), "5 - < 6", lyl_aqli_bucket), 
         lyl_aqli_bucket = ifelse((llpp_who_2021 >= 6), ">= 6 years", lyl_aqli_bucket)) %>%
  mutate(order_var = ifelse(lyl_aqli_bucket == "0 - < 0.1 years", 1, NA), 
         order_var = ifelse(lyl_aqli_bucket == "0.1 - < 0.5", 2, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "0.5 - < 1", 3, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "1 - < 2", 4, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "2 - < 3", 5, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "3 - < 4", 6, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "4 - < 5", 7, order_var), 
         order_var = ifelse(lyl_aqli_bucket == "5 - < 6", 8, order_var), 
         order_var = ifelse(lyl_aqli_bucket == ">= 6 years", 9, order_var)) 

# create a pollution version of this map with AQLI colors
aqli_2021_data_gadm2_vit_ready_shp_final_pol_view <- aqli_2021_data_gadm2_vit_ready_shp %>%
  mutate(pol_bucket = ifelse((pm2021 >= 0) & (pm2021 < 5), "0 - < 5 µg/m³", NA), 
         pol_bucket = ifelse((pm2021 >= 5) & (pm2021 < 10), "5 - < 10", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 10) & (pm2021 < 20), "10 - < 20", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 20) & (pm2021 < 30), "20 - < 30", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 30) & (pm2021 < 40), "30 - < 40", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 40) & (pm2021 < 50), "40 - < 50", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 50) & (pm2021 < 60), "50 - < 60", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 60) & (pm2021 < 70), "60 - < 70", pol_bucket), 
         pol_bucket = ifelse((pm2021 >= 70), ">= 70 µg/m³", pol_bucket)) %>%
  mutate(order_var = ifelse(pol_bucket == "0 - < 5 µg/m³", 1, NA), 
         order_var = ifelse(pol_bucket == "5 - < 10", 2, order_var), 
         order_var = ifelse(pol_bucket == "10 - < 20", 3, order_var), 
         order_var = ifelse(pol_bucket == "20 - < 30", 4, order_var), 
         order_var = ifelse(pol_bucket == "30 - < 40", 5, order_var), 
         order_var = ifelse(pol_bucket == "40 - < 50", 6, order_var), 
         order_var = ifelse(pol_bucket == "50 - < 60", 7, order_var), 
         order_var = ifelse(pol_bucket == "60 - < 70", 8, order_var), 
         order_var = ifelse(pol_bucket == ">= 70 µg/m³", 9, order_var))


# map1 (lyl view)
plt <- aqli_2021_data_gadm2_vit_ready_shp_final %>%
  select(-geometry, geometry) %>%
  st_as_sf() %>%

      ggplot() +
  geom_sf(mapping = aes(fill = lyl_aqli_bucket), color = "transparent") +
  scale_fill_manual(values = c("0 - < 0.1 years" = "#FFFFFF", "0.1 - < 0.5" = "#FFE6B3", "0.5 - < 1" = "#FFD25D", 
                                "1 - < 2" = "#FFBA00", "2 - < 3" = "#FF9600", "3 - < 4" = "#FF6908", 
                                "4 - < 5" = "#E63D23", "5 - < 6" = "#BD251C", ">= 6 years" = "#8C130E")) +
  ggthemes::theme_map() +
  labs(fill = "Gain in years of Life Expectancy", title = "AQLI 2021 data: Life years lost") +
  theme(legend.position = "bottom", 
        plot.title = element_text(size = 15, hjust = 0.5), 
        legend.justification = c(0.5, 3), 
        plot.background = element_rect(fill = "aliceblue")) +
    guides(fill = guide_legend(nrow = 1)) 

# map1 (pol 2021 view)
plt_pol_view <- aqli_2021_data_gadm2_vit_ready_shp_final_pol_view %>%
  select(-geometry, geometry) %>%
  st_as_sf() %>%
      ggplot() +
  geom_sf(mapping = aes(fill = pol_bucket), color = "transparent") +
  scale_fill_manual(values = c("0 - < 5 µg/m³" = "#FFFFFF", "5 - < 10" = "#CBE8F3", "10 - < 20" = "#B4CDD9", 
                                "20 - < 30" = "#8FA1AD", "30 - < 40" = "#707F8C", "40 - < 50" = "#576572", 
                                "50 - < 60" = "#414F5D", "60 - < 70" = "#2A3947", ">= 70 µg/m³" = "#1C2B39")) +
  ggthemes::theme_map() +
  labs(fill = "PM2.5", title = "AQLI 2021 PM2.5 pollution plot") +
  theme(legend.position = "bottom", 
        plot.title = element_text(size = 15, hjust = 0.5), 
        legend.justification = c(0.5, 3), 
        plot.background = element_rect(fill = "aliceblue"), 
        legend.key = element_rect(color = "black")) +
    guides(fill = guide_legend(nrow = 1))

ggsave(filename = "./experimentation/test_aqli_global_2021_map_feb072023_polviewinpolcolors.pdf", plot = plt_pol_view, height = 14, width = 14)

#> filtering the raster using the missing pollution data regions, and trying to convert that to 0.001

# move geometry column to the end and st_as_sf() the crap out of the missing_regions_dont_expect_missing object
missing_regions_dont_expect_missing <- missing_regions_dont_expect_missing %>%
        select(-geometry, geometry) %>% 
  st_as_sf()

# crop the population raster to the missing regions
clip_pop_raster_missing_regions <- pop_raw_landscan_crop_pol %>%
  crop(missing_regions_dont_expect_missing)

# mask the missing regions cropped population raster
mask_clip_pop_raster_missing_regions <- mask(clip_pop_raster_missing_regions, 
                                             missing_regions_dont_expect_missing)

# match the resolution of the pollution data to the population data
pol_2018_0.01_test_pop_res <- foster::matchResolution(pol_2018_0.01_test, mask_clip_pop_raster_missing_regions)

# crop the pollution data to the missing regions # 1 minute
clip_pol2018_missing_regions_shp <- pol_2018_0.01_test %>% 
  crop(missing_regions_dont_expect_missing)

# mask the raster # 3 minutes
masked_clip_pol2018_missing_regions_shp <- mask(clip_pol2018_missing_regions_shp, missing_regions_dont_expect_missing)

# resample 0.01 pollution data to 0.001
pol_2018_0.01_test_resamp_0.001 <- resample(pol_2018_0.01_test, )

#> trying something different

pop_raw_landscan_crop_pol_masked <- mask(pop_raw_landscan_crop_pol, colormap)
pol_2018_0.01_test_masked <- mask(pol_2018_0.01_test, colormap)


#> checking if rasterizing the shapefile via QGIS makes a difference (answer: very little)
test <- raster::raster("./experimentation/others/expFeb062023opointo1.tif")
test2 <- raster::raster("./experimentation/others/expFeb062023opointoo8.tif")


#> WORKS (move to main branch: aqli_workflow_vit_share_v1)!!! testing if we can capture one small region at a time (from the unexpected missing) by rasterizing it to 0.001. Starting with sharjah (I rasterized it in QGIS and seeing if it worked)

# writing uae_sharjah polygons as a shapefile
uae_sharjah <- colormap %>% filter(NAME_0 == "United Arab Emirates", NAME_1 == "Sharjah")

# reading in the QGIS rasterized version of sharjah uae version
sharjah_uae_rasterized_0.001_qgis <- raster("./experimentation/others/sharjah_uae_0.001.tif")

crs(sharjah_uae_rasterized_0.001_qgis) <- "+proj=longlat +datum=WGS84 +no_defs"

names(sharjah_uae_rasterized_0.001_qgis) <- "objid"

# crop 2018 raw pollution raster to the extent of the above raster
pol_2018_0.01_test_sharjah <- crop(pol_2018_0.01_test, extent(sharjah_uae_rasterized_0.001_qgis))

crs(pol_2018_0.01_test_sharjah) <- "+proj=longlat +datum=WGS84 +no_defs"

# match resolution of sharjah pollution raster with the 0.001 rasterized shape file
pol_2018_test_sharjah_0.001 <- raster::resample(pol_2018_0.01_test_sharjah, sharjah_uae_rasterized_0.001_qgis, method = "ngb")

names(pol_2018_test_sharjah_0.001) <- "pol"

# mask pol_2018_0.01_test_shajrah to the qgis raster
pol_2018_sharjah_0.001_test_masked <- raster::mask(pol_2018_test_sharjah_0.001, sharjah_uae_rasterized_0.001_qgis)

names(pol_2018_sharjah_0.001_test_masked) <- "pol"


# crop population raw data to sharjah
pop_raw_landscan_test_sharjah <- crop(pop_raw_landscan, extent(sharjah_uae_rasterized_0.001_qgis))

crs(pop_raw_landscan_test_sharjah) <- "+proj=longlat +datum=WGS84 +no_defs"

# match resolution of the sharjah population raster with 0.001 shapefile
pop_raw_landscan_test_sharjah_0.001 <- raster::resample(pop_raw_landscan_test_sharjah, sharjah_uae_rasterized_0.001_qgis, method = "ngb")
  
names(pop_raw_landscan_test_sharjah_0.001) <- "pop"

# mask population raw data to sharjah
pop_raw_landscan_test_sharjah_0.001_masked <- raster::mask(pop_raw_landscan_test_sharjah_0.001, 
                                                           sharjah_uae_rasterized_0.001_qgis)
names(pop_raw_landscan_test_sharjah_0.001_masked) <- "pop"

# create a brick
sharjah_brick <- sharjah_uae_rasterized_0.001_qgis %>%
  addLayer(pol_2018_sharjah_0.001_test_masked) %>%
  addLayer(pop_raw_landscan_test_sharjah_0.001_masked)

# sharjah brick df
sharjah_brick_df <- sharjah_brick %>%
  raster::as.data.frame()

# convert dataframe into an arrow table
sharjah_brick_df_arrow <- sharjah_brick_df %>%
  arrow::as_arrow_table()

# summarizing and finally testing
sharjah_brick_df_arrow_summary <- sharjah_brick_df_arrow %>%
  dplyr::filter((!is.na(objid)) & ((as.character(objid) != "NA"))) %>%
  dplyr::group_by(objid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = pop/sum(pop, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pol) %>%
  dplyr::summarise(total_population = sum(pop, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objid_gadm2 = objid)

# joining with the colormap to get area names
sharjah_brick_df_arrow_summary_shp <- sharjah_brick_df_arrow_summary %>%
  left_join(colormap, by = c("objid_gadm2" = "objectid"))

#> doing a similar test for Australia's regions where a population is not available (for Australia's Coral Sea Islands)

# na population regions
aqli_2021_data_gadm2_pop_na <- aqli_2021_data_gadm2 %>%
  filter(is.na(population))

# read in australia raster
bouvetisland_raster <- raster::raster("./experimentation/others/bouvetislandraster.tif")
names(bouvetisland_raster) <- "objid"

# crop 2018 raw pollution raster to the extent of the above raster
pol_2018_0.01_test_bouvetisland <- crop(pol_2018_0.01_test, extent(bouvetisland_raster))

crs(pol_2018_0.01_test_bouvetisland) <- "+proj=longlat +datum=WGS84 +no_defs"

# match resolution of bouvetisland pollution raster with the 0.001 rasterized shape file
pol_2018_test_bouvetisland_0.001 <- raster::resample(pol_2018_0.01_test_bouvetisland, bouvetisland_raster, method = "ngb")

names(pol_2018_0.01_test_bouvetisland) <- "pol"

# mask pol_2018_0.01_test_shajrah to the qgis raster
pol_2018_bouvetisland_0.001_test_masked <- raster::mask(pol_2018_test_bouvetisland_0.001, bouvetisland_raster)

names(pol_2018_bouvetisland_0.001_test_masked) <- "pol"

# crop population raw data to bouvetisland
pop_raw_landscan_test_bouvetisland <- crop(pop_raw_landscan, extent(bouvetisland_raster))

crs(pop_raw_landscan_test_bouvetisland) <- "+proj=longlat +datum=WGS84 +no_defs"

# match resolution of the sharjah population raster with 0.001 shapefile
pop_raw_landscan_test_bouvetisland_0.001 <- raster::resample(pop_raw_landscan_test_bouvetisland, bouvetisland_raster, method = "ngb")
  
names(pop_raw_landscan_test_bouvetisland_0.001) <- "pop"

# mask population raw data to sharjah
pop_raw_landscan_test_bouvetisland_0.001_masked <- raster::mask(pop_raw_landscan_test_bouvetisland_0.001, 
                                                           bouvetisland_raster)
# create a brick
bouvetisland_brick <- bouvetisland_raster %>%
  addLayer(pol_2018_bouvetisland_0.001_test_masked) %>%
  addLayer(pop_raw_landscan_test_bouvetisland_0.001_masked)

# sharjah brick df
bouvetisland_df <- bouvetisland_brick %>%
  raster::as.data.frame()

# convert dataframe into an arrow table
bouvetisland_brick_df_arrow <- bouvetisland_df %>%
  arrow::as_arrow_table()

# summarizing and finally testing
bouvetisland_brick_df_arrow_summary <- bouvetisland_brick_df_arrow %>%
  dplyr::filter((!is.na(objid)) & ((as.character(objid) != "NA"))) %>%
  dplyr::group_by(objid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = pop/sum(pop, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pol) %>%
  dplyr::summarise(total_population = sum(pop, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objid_gadm2 = objid)

# joining with the colormap to get area names
bouvetisland_brick_df_arrow_summary_shp <- bouvetisland_brick_df_arrow_summary %>%
  left_join(colormap, by = c("objid_gadm2" = "objectid"))

#> writing a function that would abstract away from these details
regional_summary <- function(pol_raw_raster, pop_raw_raster, region_rasterized_shapefile){
  
  # set the name of the regional rasterized shapefile
  names(region_rasterized_shapefile) <- "objid"
  
  # crop 2018 raw pollution raster to the extent of the above raster
 pol_raster_cropped <- crop(pol_raw_raster, extent(region_rasterized_shapefile))

 # set crs of the pollution raster
crs(pol_raster_cropped) <- "+proj=longlat +datum=WGS84 +no_defs"

# match resolution of pollution raster with the rasterized regionshape file
pol_raster_cropped_new_res <- raster::resample(pol_raster_cropped, region_rasterized_shapefile, method = "ngb")

names(pol_raster_cropped_new_res) <- "pol"

# mask pol_2018_0.01_test_shajrah to the qgis raster
pol_raster_cropped_new_res_masked <- raster::mask(pol_raster_cropped_new_res, region_rasterized_shapefile)

# set the name of the pollution raster
names(pol_raster_cropped_new_res_masked) <- "pol"

# crop population raw data to region rasterized shapefile
pop_raster_cropped <- crop(pop_raw_raster, extent(region_rasterized_shapefile))

crs(pop_raster_cropped) <- "+proj=longlat +datum=WGS84 +no_defs"

# replacing the population with population densities
# raster::values(pop_raster_cropped) <- as.vector(pop_raster_cropped * (0.01/0.855625)) 
raster::values(pop_raster_cropped) <- as.vector(pop_raster_cropped * 0.01440115)

# performing idw on population densities

#match resolution of the pop_raster_cropped population raster with region_rasterized_shapefile
pop_raster_cropped_new_res <- raster::resample(pop_raster_cropped, region_rasterized_shapefile, method = "ngb")
  

# mask population raw data to sharjah
pop_raster_cropped_new_res_masked <- raster::mask(pop_raster_cropped_new_res, 
                                                          region_rasterized_shapefile)

# pop_raster_cropped_new_res
names(pop_raster_cropped_new_res_masked) <- "pop"

# create a brick
region_brick <- region_rasterized_shapefile %>%
  addLayer(pol_raster_cropped_new_res_masked) %>%
  addLayer(pop_raster_cropped_new_res_masked)

# sharjah brick df
region_brick_df <- region_brick %>%
  raster::as.data.frame()

# convert dataframe into an arrow table
region_brick_df_arrow <- region_brick_df %>%
  arrow::as_arrow_table()

# summarizing and finally testing
region_brick_df_arrow_summary <- region_brick_df_arrow %>%
  dplyr::filter((!is.na(objid)) & ((as.character(objid) != "NA"))) %>%
  dplyr::group_by(objid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = pop/sum(pop, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pol) %>%
  dplyr::summarise(total_population = sum(pop, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::rename(objid_gadm2 = objid)

# joining with the colormap to get area names
region_brick_df_arrow_summary_shp <- region_brick_df_arrow_summary %>%
  left_join(colormap, by = c("objid_gadm2" = "objectid"))

return(region_brick_df_arrow_summary_shp)
  
}

colormap %>% 
  filter(NAME_0 == "Australia", 
         NAME_1 == "Queensland", 
         NAME_2 == "Wujal Wujal") %>%
  st_write("./experimentation/others/wujalwujal.shp")

region_rasterized_shapefile <- raster::raster("./experimentation/others/gujarat_tif.tif")


# rasterizing a subset of the colormap shapefile to 0.001 using fasterize: test (Gujarat): Works
gujarat_raster_r_0.001_test <- fasterize(colormap %>% filter(NAME_0 == "India", NAME_1 == "Gujarat"), raster::raster(ext = raster::extent(colormap %>% filter(NAME_0 == "India", NAME_1 == "Gujarat")), resolution = 0.001, crs = crs(colormap)), field = "objectid", fun = "last")

# rasterizing a subset of the colormap shapefile to 0.001 using fasterize: test (Illinois): Works
illinois_raster_r_0.001_test <- fasterize(colormap %>% filter(NAME_0 == "United States", NAME_1 == "Illinois"), raster::raster(ext = raster::extent(colormap %>% filter(NAME_0 == "United States", NAME_1 == "Illinois")), resolution = 0.001, crs = crs(colormap)), field = "objectid", fun = "last")

# rasterizing a subset of the colormap shapefile to 0.001 using fasterize": test (Wujal Wujal)
wujalwujal_raster_r_0.001_test <- fasterize(colormap %>% filter(NAME_0 == "Australia", NAME_2 == "Wujal Wujal"), raster::raster(ext = raster::extent(colormap %>% filter(NAME_0 == "Australia", NAME_2 == "Wujal Wujal")), resolution = 0.001, crs = crs(colormap)), field = "objectid", fun = "last")

# test
foo <- regional_summary(pol_2018_0.01_test, pop_raw_landscan, region_rasterized_shapefile)
foo1 <- regional_summary(pol_2018_0.01_test, pop_raw_landscan, gujarat_raster_r_0.001_test)
foo2 <- regional_summary(pol_2018_0.01_test, pop_raw_landscan, wujalwujal_raster_r_0.001_test)
foo3 <- regional_summary(pol_2018_0.01_test, pop_raw_landscan, illinois_raster_r_0.001_test)



```



# tmp 

```{r}

```

