---
title: "Workflow (building on August 2023 report pipeline) for Sept-2024 report"
author: "Aarsh (aarshbatra@uchicagotrust.org/aarshbatra.in@gmail.com)"
date: '2023-10-22'
output: html_document
---

# setup
```{r setup}
knitr::opts_chunk$set(echo = FALSE)
# start time
start_time <- Sys.time()

# load libraries
library(raster)
library(rgdal)
library(dplyr)
library(readr)
library(ncdf4)
library(assertthat)
library(fasterize)
library(sf)
library(SpaDES)
library(foster) # for matching resolution of 2 different rasters
library(DBI) # for connecting R with Postgres
library(RPostgres)
library(ggplot2)
library(RPostgres)
# library(sparklyr)
library(data.table)
library(stringr)
library(here)

# global variables (check for updates, if any)
who_pm2.5_standard <- 5 # in micrograms per cubic meter, annual average PM2.5 standard
aqli_lyl_constant <- 0.098
india_pm2.5_standard <- 40 # in micrograms per cubic meter
region_pm2.5_standard <- 15 # China

# helpful functions
`%notin%` <- Negate(`%in%`)
# source(paste0(here(), "/R/sat.data.processing.general.helper.R"))

print("Libraries and Global variables loaded in.")

```

#> Using the new workflow, generate gadm level 2 population weighted pollution and life years lost numbers (WHO and National Standard) for a given year's Global data. Standalone, uses its own data files, not the data files from the second chunk up top.

# set paths

```{r set_paths, echo=FALSE}

#> paths and global variables (create the necessary folder structure after reading through the paths section. All paths are relative to your current working directory, but you might need to make a few folders, for e.g. for specific resolution datasets). After that, run this script and everything should run smoothly.----------------------------------------

# master folder name
master_folder_data_path <- "ar.2024.update.using.2022.pol.data"

# pollution
pol_data_location <- paste0(here(), "/", master_folder_data_path, "/", "data/input/pollution/0.01x0.01/GWRPM25-NoDust-NoSeaSalt_0.01_0.01/GWRPM25-NoDust-NoSeaSalt/Annual/")



# population
pop_data_location <- paste0(here(), "/",  master_folder_data_path, "/data/", "input/population/")
pop_data_file_name <- "landscan-global-2022.tif"



#> shapefiles

# general shape file folder location
shp_files_location <- paste0(here(), "/", master_folder_data_path, "/",  "data/intermediate/1_population_and_colormap/1_shapefile_aggregate/")

# gadm2 shape file location
colormap_location <- "colormap/colormap.shp"

# hover map shape file location (not using this year)
hovermap_location <- "hover/hover.shp"

# gadm0 shape file location
gadm0_shp_file_location <- "colormap_collapsed_gadm0/aqli_gadm2_collapse_to_gadm0.shp" 

# gadm1 shape file location
gadm1_shp_file_location <- "colormap_collapsed_gadm1/aqli_gadm2_collapse_to_gadm1.shp" 


#> raster resolution of the final data brick (containing a rasterized pollution, population and a rasterized shape file)
raster_res <- 0.008

# data timeline (note)
pol_data_start_year <- 1998
update_year <- 2022

# data levels
gadm0_folder_name <- "gadm_0"
gadm1_folder_name <- "gadm_1"
gadm2_folder_name <- "gadm_2"

# corresponnding aqli report publishing year (this is the year in which "update_year"'s data will be published. Current lag is 2 years).
report_publishing_year <- 2024

# ssd aqli folder high res (0.01 as in December, 2022) location (use ssd for high writing speeds, hdd's suck)

# drive location (external ssd used, because this ThinkPad only has 500 GB ssd, and it'll get almost full, so use an external ssd)
ssd_drive <- paste0(here())

# data folder name (on ssd)
aqli_data_share_folder_name <- "aqli.2024.report.data.share"

# high res data location
ssd_location_rasterized_data_0.008 <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".data", "/rasterized/", raster_res, "/", sep = "")

# updated national standards file location and file name
national_standards_pm2.5_location <- paste0(here(), "/", master_folder_data_path, "/", "data/input/standards/") 

national_standards_pm2.5_sep_2023_file_name <- "country_annual_average_pm2.5_standards_asInSep2023.csv"


#> collapsed data path: gadm0 level
ssd_location_collapsed_gadm0_data_path <- stringr::str_c(ssd_drive,"/", aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm0_folder_name, "/", sep  = "")

#> collapsed data path: gadm1 level
ssd_location_collapsed_gadm1_data_path <- stringr::str_c(ssd_drive,"/", aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm1_folder_name, "/", sep  = "")

#> collapsed data path: gadm2 level
ssd_location_collapsed_gadm2_data_path <- stringr::str_c(ssd_drive,"/", aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", gadm2_folder_name, "/", sep  = "")

#> na population regions processed folder path

na_pop_regions_folder_name <- "na_population_gadm2_regions_resamp_to_0.001_processed"

na_pop_regions_folder_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", na_pop_regions_folder_name , "/", sep  = "")

#> missing regions processed folder path
missing_regions_folder_name <- "missing_gadm2_regions_resamp_to_0.001_processed"

missing_regions_folder_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", missing_regions_folder_name , "/", sep  = "")

#> pov files folder path

pov_files_folder_name <- "pov_files"

pov_files_folder_path <- stringr::str_c(ssd_drive, aqli_data_share_folder_name, "/", report_publishing_year, ".publish.Year.with.", update_year, ".", "data", "/collapsed/", pov_files_folder_name , "/", sep  = "")

#-------------------------------------------------

```



# main pipeline to get yearly gadm2 and high res pollution datasets

```{r get_yearly_pol_datasets, echo=FALSE}

# benchmarking
threshold_0 <- Sys.time() 

# population raw data
population_dataset <- raster::raster(str_c(pop_data_location, pop_data_file_name, sep = ""))

# close connection
closeAllConnections()

# naming the raw global landscan population data and setting its crs to be the same as the pollution data
raster::crs(population_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(population_dataset) <- "population"


#--- no longer need to crop, as this can be reused, so I wrote it as a tif, and then will simply read it
#- crop population dataset to the pollution dataset
 # pollution_dataset_name_tmp <- "V5GL04.HybridPM25-NoDust-NoSeaSalt.Global.202201-202212.nc"
 # pollution_dataset_tmp <- raster::raster(str_c(pol_data_location, pollution_dataset_name_tmp, sep = ""))
 # pop_raw_landscan_crop_pol <- raster::crop(population_dataset, pollution_dataset_tmp)

#- write the cropped population dataset
 # pop_raw_landscan_crop_pol %>%
 #   raster::writeRaster(filename = paste0(here(), "/", master_folder_data_path, "/data/intermediate/pop_raw_landscan_crop_pol.tif"), 
 #                       format = "GTiff", overwrite = TRUE)
#---

# reading in the pre-cropped population raster, which remains the same for all pollution datasets
pop_raw_landscan_crop_pol <- raster::raster(paste0(here(), "/", master_folder_data_path, "/data/intermediate/pop_raw_landscan_crop_pol.tif"))

# close connection
closeAllConnections()


# load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))


#-- this remains same for all pollution datasets, hence writing it, and will then simply read it
polygon_cells <- fasterize(colormap, pop_raw_landscan_crop_pol , field = "objectid", fun = "last")
writeRaster(polygon_cells,filename = "./experimentation/colormap_rasterized.tif",format = "GTiff", overwrite = TRUE)
#---

polygon_cells <- raster::raster(paste0(here(), "/", "experimentation/colormap_rasterized.tif"))

# close connection
closeAllConnections()

# making sure that the crs of the rasterized shapefile is the same as the 0.008 population and pollution rasters. 
raster::crs(polygon_cells) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

# benchmarking
threshold_1 <- Sys.time() 

print("All raw datasets read into R.")

#> reading pollution data, one year at a time and then will concatenate all results-----------------------
# Note that the datasets in this pipeline do not include the  geometry column. That can be added in the end after concatenating all yearly datasets into a single final gadm2 dataset. For the gadm0 and gadm1 datasets (which will be derived from the single "final gadm2 dataset"), a similar process will follow.

# pol data list
pol_data_list <- list.files(pol_data_location) %>% sort()

# pollution column names empty vector
pol_col_name_vec <- c()

#> for loop begins----------------------------------------------------------------

print("For loop begins: Processing pollution rasters 1 year at a time")

for (i in 1:length(pol_data_list)){
  
    # for testing purposes

   if(i <= 22){
     next
   }
  
  # close all connections
  closeAllConnections()
  
  # benchmarking
  threshold_1.5 <- Sys.time()
  
  print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " begins"))
  
  pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
  
  # pollution file name given the current iteration
  cur_pol_file_name <- pol_data_list[i]
  
  # cur pollution file year
  cur_pol_file_year <- stringr::str_extract(str_extract(cur_pol_file_name, "(\\d+)-(\\d+)"), "....")
  
  # read in the pollution raster for a given year
  pollution_dataset <- raster::raster(str_c(pol_data_location, cur_pol_file_name, sep = ""))
  
  # close connection
  closeAllConnections()
  
  # naming the raw global pollution data and setting its crs
raster::crs(pollution_dataset) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
names(pollution_dataset) <- "pm2.5_pollution"


# benchmarking
threshold_2 <- Sys.time() 

# matching the resolution of the cropped population and pollution datasets
pol_0.01_region_in_landscan_pop_res <- foster::matchResolution(pollution_dataset, pop_raw_landscan_crop_pol)

# benchmarking
threshold_3 <- Sys.time() 

print("stacking all layers in a raster brick")

# creating a raster brick using the population and pollution data
region_raster_brick <- pop_raw_landscan_crop_pol %>% 
  raster::addLayer(pol_0.01_region_in_landscan_pop_res) 

# close connection
closeAllConnections()

# setting the names of the newly created placheolders
names(region_raster_brick) <- c("population", "pm2.5_pollution")

# set the same crs for pollution brick
raster::crs(region_raster_brick) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

print("Population and pollution layers matched")

# benchmarking
threshold_4 <- Sys.time() 

# Now match each population/pollution point to a colormap polygon. To do this, convert
# polygons to raster of same resolution as population raster, with value of each cell equal
# to objectid of polygon that covers its center.
# Fasterize is an ultra-fast version of the rasterize function.

# add rasterized colormap to the raster brick
region_raster_brick <- region_raster_brick %>% 
  raster::addLayer(polygon_cells)

print("added rasterized colormap layer to the brick")

names(region_raster_brick)[length(names(region_raster_brick))] <- "colormap_objectid"

# benchmarking
threshold_5 <- Sys.time() 

# from this point forward, replace all instances of "region_raster_brick_df" with "aqli_raster_brick_df". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_df <- raster::as.data.frame(region_raster_brick)

print("raster brick converted to data frame")

# write rasterized dataframe to ssd, in arrow data format (0.008x0.008 resolution)
# aqli_raster_brick_df %>% arrow::write_dataset(str_c(ssd_location_rasterized_data_0.008, cur_pol_file_year, ".parquet"))

# benchmarking
threshold_6 <- Sys.time() 

#-- (Update: no longer needed as we directly coerce region_raster_brick_df to an arrow table). Will only need to write this when implementing high res layer.
#- write region_raster_brick_df to a csv
#--

# from this point forward, replace all instances of "pollution_data_0.01_light" with "aqli_raster_brick_light". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_raster_brick_light <- arrow::as_arrow_table(aqli_raster_brick_df)

# benchmarking
threshold_7 <- Sys.time() 

print("rasterized dataframe coerced to an arrow table")

#-- read raster data using arrow (no longer needed as we already coerced region_raster_brick_df to an arrow table)
# aqli_raster_brick_light <- arrow::open_dataset("./experimentation/pollution_data_0.01_2021.csv", format = "csv")
#-- 


# from this point forward, replace all instances of "pollution_district_wise" with "aqli_gadm2_collapse". Make sure that to make this update in all previous branches. If you are reading this, and if other branches still exist at this point. Make sure to make this update in those branches (even though you might end up using just this branch, its good to make that change).
aqli_gadm2_collapse <- aqli_raster_brick_light %>%
  dplyr::filter((!is.na(colormap_objectid)) & ((as.character(colormap_objectid) != "NA"))) %>%
  dplyr::group_by(colormap_objectid) %>%
  dplyr::collect() %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE), 
         pollution_pop_weighted = pop_weights*pm2.5_pollution) %>%
  dplyr::summarise(total_population = sum(population, na.rm = TRUE), 
            avg_pm2.5_pollution = sum(pollution_pop_weighted, na.rm = TRUE)) %>%
  dplyr::ungroup() %>%
  dplyr::rename(objectid_gadm2 = colormap_objectid)

# renaming the pollution column such that it includes the year in question
colnames(aqli_gadm2_collapse)[str_detect(colnames(aqli_gadm2_collapse), "avg_pm2.5_pollution")] <- pol_col_name_vec[i]

# writing the gadm2 level dataset to ssd
aqli_gadm2_collapse %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, cur_pol_file_year, "_", gadm2_folder_name, ".csv"))

# benchmarking
threshold_8 <- Sys.time() 



print(stringr::str_c("Iteration #", i, "/", (update_year - pol_data_start_year) + 1, " end"))

print("Freeing memory before proceeding to next iteration")

# benchmarking
threshold_9 <- Sys.time()

# collect garbage, free memory
gc()

# benchmarking
threshold_10 <- Sys.time()

# time it took to complete the current iteration

print(str_c("Time taken to complete iteration # ", i, "/", (update_year - pol_data_start_year) + 1, ": ", threshold_10 - threshold_1.5))
closeAllConnections()
closeAllConnections()

}

end_time_main_pipeline <- Sys.time()
time_diff <- end_time_main_pipeline - start_time

print(str_c("Total time taken (main pipeline for loop) with ", i, " years of pollution data processed: ", time_diff))

#> for loop ends-----------------------------------------------------------------------


```

# combine all pollution yearly datasets into a single gadm2 dataset, in the format that has to be shared with VIT. See AQLI data dictionary for more information.

```{r combine_yearly_pol_data, echo=FALSE}

#---------
#> Note: if you have already processed the pollution datasets in the above chunk and just want to run the combining code in this chunk, make sure to uncomment the following code and run it before proceeding to get the colormap and pol_col_name_vec. Also, make sure to run the "setup" and "set path" chunks. If you have already run the above chunks, then you would already have these objects and in that case there is no need to run the below commented code.

# # load latest colormap shapefile for (last complete updated: November, 2022)
colormap <- sf::st_read(str_c(shp_files_location, colormap_location, sep = ""))

# generate the pol_col_name_vec
pol_col_name_vec <- c()
for(i in 1:25){
     pol_col_name_vec[i] <- str_c("pm", (pol_data_start_year + (i-1)))
 }

#-----------

# benchmarking
threshold_11 <- Sys.time() 

#> Output in VIT data sharing format

# read in all pollution data and name_0, name_1, name_2 columns from colormap (joined in the first iteration of the loop below) into a single dataset, with just pollution columns. I am in the process of updating national standards, so for now we have a placeholder national standards column, which is just set to 10 micrograms per cubic meter for all regions. The updated national standards column will be added alongside the life years lost columns after the below for loop.

# list of files in the collapsed folder
collapsed_gadm2_files_vec <- list.files(str_c(ssd_location_collapsed_gadm2_data_path)) %>% 
  sort() 

# indices of the relevant files (from the above vector) needed for combining.
collapsed_gadm2_files_vec_rel_ind <- str_detect(collapsed_gadm2_files_vec, ".csv")

# keeping only relevant files that will be combined below
collapsed_gadm2_files_vec_rel_files <- collapsed_gadm2_files_vec[collapsed_gadm2_files_vec_rel_ind]

for(i in 1:length(collapsed_gadm2_files_vec_rel_files)){
  
    
  if(i == 1){
    temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))

    aqli_gadm2_collapse_master <- temp_gadm2 %>%
      dplyr::left_join(colormap, by = c( "objectid_gadm2"="objectid", "iso_alpha3" = "iso_alpha3")) %>%
      dplyr::mutate(whostandard = 5, 
             natstandard = 10) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, NAME_0, NAME_1, NAME_2, population,
             whostandard, natstandard,
             pol_col_name_vec[i]) %>%
      dplyr::rename(country = NAME_0,
             name_1 = NAME_1, 
             name_2 = NAME_2)
           #  ,population = total_population)
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
    
  } else{
    #temp_gadm2 <- readr::read_csv(str_c(ssd_location_collapsed_gadm2_data_path, collapsed_gadm2_files_vec_rel_files[i]))
    
    aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>% 
      dplyr::left_join(temp_gadm2, by  = c("objectid_gadm2", "iso_alpha3" )) %>%
      #dplyr::select(-c(total_population)) %>%
      dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, pol_col_name_vec[1:(i-1)],
             pol_col_name_vec[i])
    
    colnames(aqli_gadm2_collapse_master)[ncol(aqli_gadm2_collapse_master)] <- pol_col_name_vec[i]
  
  }
}

#> write aqli_gadm2_collapse_master df's current version (which does not include the missing regions and also does not capture the na pop regions that we will do next). Next step will be to capture the na population regions (by that I don't mean regions with NA in the population column (no such regions exist), but rather regions where population = 0) and missing regions and then finally we'll incorporate them in the aqli_gadm2_collapse_master and write that final version later.

aqli_gadm2_collapse_master %>%
    readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, "[missingAndNAPopRegionsNotYetIncorporatedOnlyPMVersion]master_global_allyears_gadm2_non_geom.csv"))

aqli_gadm2_collapse_master <- read_csv(str_c(ssd_location_collapsed_gadm2_data_path, "[missingAndNAPopRegionsNotYetIncorporatedOnlyPMVersion]master_global_allyears_gadm2_non_geom.csv"))

#> missing regions and zero population gadm2 capture using the regional_summary function (by resampling to a resolution of 0.001) from the helper file in the R sub-directory and adding them back into the aqli_gadm2_collapse_master object, before continuing to add life years lost columns to it.------------------

#aqli_gadm2_collapse_master <- read.csv(str_c(ssd_location_collapsed_gadm2_data_path,"[missingAndNAPopRegionsNotYetIncorporatedOnlyPMVersion]master_global_allyears_gadm2_non_geom.csv"))

# If, not running the pipeline from scratch, use the already written gadm2_aqli dataset and uncomment the below block, otherwise use aqli_gadm2_collapse_master.

# gadm2_aqli_2021 <- readr::read_csv("./ar.2023.update.using.2021.pol.data/data/output_vit/gadm2_aqli2021_vit.csv")

# NA pop regions object ids
# objectid_gadm2_na_pop <- gadm2_aqli_2021 %>%
#   filter(is.na(population)) %>%
#   pull(objectid_gadm2) %>%
#   as.vector()

# NA pop regions object ids
objectid_gadm2_na_pop <- aqli_gadm2_collapse_master %>%
  filter(population == 0) %>%
  pull(objectid_gadm2) %>%
  as.vector()

# processing all NA pop regions and outputting a single dataset for all regions containing pollution info for all years

res_resample_to <- 0.001 # mention the resolution to which the na pop region is to be resampled.
res_resample_from <- 0.00833333 # mention the current resoltion 
start_year <- 1998 # mention start year, usually the same as the first year for which the AQLI data is available.

# capturing zero pop regions in batches (the cuts will have to be experimented with because sometimes specific regions take more time than others. I am not entirely sure, why that is the case as of now, but that is something that can be looked into. For example, it's not necessarily the case that regions with bigger areas take more time to process, sometimes, regions with relatively smaller areas take more time to process compared to regions with bigger areas)
na_pop_regions_processed_part1_1_23 <- regional_summary(colormap, pol_data_location, pop_raw_landscan_crop_pol, res_resample_to,objectid_gadm2_na_pop[1:23], start_year, res_resample_from)

na_pop_regions_processed_part2_24_24 <- regional_summary(colormap, pol_data_location, pop_raw_landscan_crop_pol, res_resample_to,objectid_gadm2_na_pop[24], start_year, res_resample_from)

na_pop_regions_processed_part3_25_60 <- regional_summary(colormap, pol_data_location, pop_raw_landscan_crop_pol, res_resample_to,objectid_gadm2_na_pop[25:60], start_year, res_resample_from)

na_pop_regions_processed_part4_61_end <- regional_summary(colormap, pol_data_location, pop_raw_landscan_crop_pol, res_resample_to,objectid_gadm2_na_pop[61:length(objectid_gadm2_na_pop)], start_year, res_resample_from)

# bind rows to combine our attempt to capture na pop regions
na_pop_regions_processed <- bind_rows(na_pop_regions_processed_part1_1_23, na_pop_regions_processed_part2_24_24, 
                                      na_pop_regions_processed_part3_25_60, na_pop_regions_processed_part4_61_end)


# adding a temporary national standard column to the na_pop_regions_processed dataset so that it matches the aqli_gadm2_collapse_master dataset

na_pop_regions_processed <- na_pop_regions_processed %>%
  mutate(natstandard = NA) %>%
  select(objectid_gadm2:whostandard, natstandard, everything())

# if for a given region pop still = 0, replace all other pm columns with NA and then replace the pop column with NA
na_pop_regions_processed_final <- na_pop_regions_processed %>%
  dplyr::mutate(across(starts_with("pm"), ~ifelse(population == 0, NA, .x))) %>%
  mutate(population = ifelse(population == 0, NA, population))
  

# write the na pop regions processed (22 out of 102 were processed, but we write all 102 to the ssd) df to ssd's relevant folder

na_pop_regions_processed_final %>% 
  write_csv(str_c(na_pop_regions_folder_path, "na_pop_regions_processed.csv"))

## filter out those regions whose population doesn't make sense (limitation of the current resampling pipeline, need to resolved in future updates)

# # calculating area of the 97 na population regions (sanity check)
# na_pop_regions_colormap <- colormap %>%
#   filter(objectid %in% objectid_gadm2_na_pop)
# 
# # calculating area for the above regions, after transforming to crs 4326
# na_pop_regions_colormap <- st_transform(na_pop_regions_colormap, 4326)
# na_pop_regions_colormap <- st_make_valid(na_pop_regions_colormap)
# 
# # creating a new column that will measure area in square kilometers
# na_pop_regions_colormap$area <- as.numeric(st_area(na_pop_regions_colormap)/(10^6))
# 
# # creating a new column that displays area without units
# na_pop_regions_colormap <- na_pop_regions_colormap %>%
#   mutate(area_sans_units = as.numeric(area)) %>%
#   st_drop_geometry()


## post finalizing the na pop regions that are to be included, now getting the missing regions

# read in 2021 processed aqli gadm2 data (only do this if running this specific bit of the pipeline), otherwise use the aqli_gadm2_collapse_master dataset
# aqli_2021_gadm2_test <- read_csv("./ar.2023.update.using.2021.pol.data/data/output_vit/gadm2_aqli2021_vit.csv")

# get indices for missing areas
missing_ids_index <- colormap$objectid %notin% aqli_gadm2_collapse_master$objectid_gadm2

# missing areas (153 in total)
missing_areas <- colormap[missing_ids_index, ]

# calculating areas for missing areas and filetering areas whose area is less than or equal to  a threshold
missing_areas <- st_transform(missing_areas, 4326)
missing_areas <- st_make_valid(missing_areas)

# creating a new column called area
missing_areas$area <- as.numeric(st_area(missing_areas)/(10^6))

# create a non-geom version of the dataset
missing_areas_drop_geom <- missing_areas %>%
  mutate(area_sans_units = as.numeric(area)) %>%
  st_drop_geometry()

# regions that we expect to be missing and filtering them out of the above list (75 out of 153 total missing)
missing_regions_exp_to_be_missing <- readxl::read_xlsx(paste0(here(), "/", master_folder_data_path, "/data/intermediate/", "part-1_expectToBeMissing.xlsx"))

# # regions we don't expect to be missing according to the csv (the method right below, is more accurate way to get the 153 - 70 regions that we don't expect to be missing).
# missing_regions_dont_expect_missing <- missing_areas_drop_geom %>%
#   filter(NAME_0 %notin% missing_regions_exp_to_be_missing$NAME_0 & 
#         NAME_1 %notin% missing_regions_exp_to_be_missing$NAME_1 &
#         NAME_2 %notin% missing_regions_exp_to_be_missing$NAME_2)

# regions that we don't expect to be missing (78 = 153 - 75, where 75 are the ones we expect to be missing) according to the object ids (this will give us the 83 regions that we don't expect to be missing). Post that, I'll remove a 5 other regions by feeding in object ids that I know are outside the bounds of the ACAG pm2.5 data limits (). As of now, this part of code requires human intervention and a bit of testing around (every year this bit would have to be manually changed, e.g. the area threshold in the filter command below), but I am thinking of a better way to automate this.
missing_regions_dont_expect_missing_objid <- missing_areas_drop_geom %>%
  filter(area_sans_units < 245.9) %>%
  filter(objectid %notin% c(28044, 28048, 28063, 35561, 35567)) %>%
  pull(objectid) %>%
  as.vector()

# capture missing regions and add a temporary natstandard column that will be updated later on
missing_regions_capture <- regional_summary(colormap, pol_data_location, pop_raw_landscan_crop_pol, res_resample_to,missing_regions_dont_expect_missing_objid, start_year, res_resample_from)

missing_regions_capture <- missing_regions_capture %>%
    mutate(natstandard = NA) %>%
  select(objectid_gadm2:whostandard, natstandard, everything())

missing_regions_capture_final <- missing_regions_capture %>%
   dplyr::mutate(across(starts_with("pm"), ~ifelse(population == 0, NA, .x))) %>%
  mutate(population = ifelse(population == 0, NA, population))
 

# missing regions captured write to ssd
missing_regions_capture_final %>%
   write_csv(str_c(missing_regions_folder_path, "missing_regions_processed.csv"))

## store objectids of all resampled/captured (to a resolution of 0.001) na population regions in a vector (already present in the gadm2 collapsed file and will replace their old version). 16/97 na pop regions have been captured. For the ones, not captured, improvements will be made to this pipeline in the future. The reason might be that to capture the coordinates more precisely (more decimal points) because when capturing tiny regions, this level precision matters. All in all 99.8089% of the regions have been captured. A couple notes (on some regions that were manually dropped): na_pop_regions_dropped_obj_ids (if no specific comments mentioned, means population is 0 and as a result pollution was 0): 25050 (pop avialable, but all pollution  columns are 0, weird), 35356 (population is 0, but all pollution columns are available, more weirder), 37791 (population is available, but all pollution data columns are missing), 42205 (zero population, but all pollution columns are available). Only kept rows where both population and atleast one pollution column is available.

## store objectids of all resampled (to a resolution of 0.001) missing regions, that we don't expect to be missing, in a vector (not present in the current gadm2 collapsed file, and will be appended to the file after na pop regions have been replaced. Post that sort by objectid_gadm2). 47/83 missing regions (that we don't expect to be missing) have been captured. The remaining 70 regions, we do expect to be missing are missing because they are either above ~67 degree N, or below ~ 55 degree south. Again, for the ones, not captured, improvements will be made to this pipeline in the future. The reason might be that to capture the coordinates more precisely (more decimal points) because when capturing tiny regions, this level precision matters. A couple notes on some regions that were manually dropped: missing_regions_non_zero_pop_dropped_obj_ids: 2079 (population available, but all pollution columns are 0), 25046 (population available, but all pollution columns are 0), 29681 (population available, but all pollution columns are 0), 34362 (population is 0, but all pollution columns are available), 34646 (pop was 13, pollution data was available, but pop was too low), 34834 (pop was 3 and pollution data is available, but pop was too low), 34884 (pop was 9 and pollution data was available, but population value is too low), 34893 (pop was 2 and pollution columns are available, but population value was too low), 35019 (pop was 8, pollution column data are available, but pop was too low), 35024 (0 pop, but all pollution columns are available), 35160 (0 pop, but all pollution columns are available), 35582 (0 pop, but all pollution columns are available), 35016 (pop was avail, but all pollution columns are 0), 35589 (pop 56, all pollution columns available, but pop was too low), 35603 (pop 1, all pollution columns available, but pop was too low), 36393 (pop 1, all pollution columns available, but pop was too low), 36436 (pop 4, all pollution columns available, but pop was too low), 36828 (pop was 8, and all pollution columns are available, but pop was too low). Only kept rows where both population and atleast one pollution column is available.

# The regions that were manually dropped have been explained below.

# a subset of the above na pop regions and missing regions that have been saved have populations that are not captured very well, open those csv's in excel and browse through them and get rid of populations that seem super unreasonable (e.g. a population of 1 for a huge region). This part as of now is manual, but I am thinking of ways to automate it. It's not just that rows with population below a certain number can be eliminated, it's more subtle than that, so need to think about that a little and implement a more elegant way to do this bit in the future. Note that, in the na pop regions and missing regions folder on the ssd, there will be 2 versions of each. One with all regions, and the other one (will have the word "handpicked" in the file name), with handpicked regions. Read in the handpicked regions below.

# Before the incorporation of missing and na pop regions we had 48059 unique object ids in gadm 2 (including 97 na pop regions). Now removing the 97 na pop regions and replaing them with 16 filled pop regions and 47 regions that were previously missing, brings the final count of object ids to 48025 (48059 - 97 + 16 + 47). All in all 99.61213% (48025/48212) of the regions have been captured, where 48212 are the total number of objectids in the latest colormap shapefile.

# na_pop_regions_processed_handpicked <- read_csv(str_c(na_pop_regions_folder_path, "na_pop_regions_processed_handpicked.csv"))
# 
# missing_regions_processed_handpicked <- read_csv(str_c(missing_regions_folder_path, "missing_regions_processed_handpicked.csv"))
# 
# 
# na_pop_regions_processed_handpicked_obj_ids <- na_pop_regions_processed_handpicked$objectid_gadm2



#> incorporate all missing and na pop regions here (even the ones that we expect to be missing, i.e. all 48212 regions)-------------

# first capture all regions that we don't expect to be missing
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  filter(objectid_gadm2 %notin% na_pop_regions_processed_final$objectid_gadm2) %>%
  bind_rows(na_pop_regions_processed_final) %>%
  bind_rows(missing_regions_capture_final) %>%
  arrange(objectid_gadm2)

# df of regions that we expect to be missing

col_names_tmp <- str_c("pm", 1998:2022)

 df_regions_exp_to_be_missing <- colormap %>% 
  anti_join(tmp, by = c("objectid" = "objectid_gadm2")) %>%
  st_drop_geometry() %>%
    mutate(whostandard = 5, natstandard = NA) %>%
  select(objectid, iso_alpha3, NAME_0, NAME_1, NAME_2, whostandard, natstandard) %>%
  rename(objectid_gadm2 = objectid, country = NAME_0, 
         name_1 = NAME_1, name_2 = NAME_2) 
 
 df_regions_exp_to_be_missing[col_names_tmp] <- NA
 
# bind rows with the master dataset 
 
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  bind_rows(df_regions_exp_to_be_missing) %>%
  arrange(objectid_gadm2)


#> removing waterbodies from the gadm2 dataset and post this gadm2 data will be finalized (do bear lake adjustment)-----------------------------------

# read in waterbodies file (Bear Lake is not counted in this, because that's a county in Idaho, US)
waterbodies_aqli <- read_csv(paste0(here(), "/", master_folder_data_path, "/", "data/input/others/waterbodies_aqli.csv")) 

waterbodies_aqli <- waterbodies_aqli %>%
  select(-type)


# filter waterbodies in the aqli gadm2 collapse master dataset and then 2 extra rows after that don't get proper matched
waterbodies_aqli_final <- aqli_gadm2_collapse_master %>% 
  filter(country %in% waterbodies_aqli$gadm0 & name_1 %in% waterbodies_aqli$gadm1, name_2 %in% waterbodies_aqli$gadm2)

# remove these rows
two_extra_rows <- tmp %>% 
  anti_join(waterbodies_aqli, by = c("country" = "gadm0", "name_1" = "gadm1", "name_2" = "gadm2"))

# final waterbodies to be removed
waterbodies_aqli_final <- waterbodies_aqli_final %>%
  filter(objectid_gadm2 %notin% two_extra_rows$objectid_gadm2)

# remove the waterbodies from the aqli gadm2 collapse master
aqli_gadm2_collapse_master <-  aqli_gadm2_collapse_master %>%
  filter(objectid_gadm2 %notin% waterbodies_aqli_final$objectid_gadm2)


#-----------------------------------------------------------------

# benchmarking
threshold_12 <- Sys.time() 

#> add in the updated national standards column (last updated: January 18, 2023 by Aarsh) and bring it in appropriate "join ready" format (to be joined with the combined pollution data before adding life years lost columns)

# reading in the national standards file (to do: add in (if missing) the national standards of the missing and zero pop regions captured above.)
national_standards_pm2.5 <- readr::read_csv(stringr::str_c(national_standards_pm2.5_location, national_standards_pm2.5_sep_2023_file_name, sep = ""))

# keeping and renaming relevant columns
national_standards_pm2.5 <- national_standards_pm2.5 %>%
  dplyr::select(country, nat_ann_avg_pm2.5_aqli) 

# replacing the national standards column of the aqli_gadm_collapse_master file (output of the above for loop) with the updated national standards column
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::left_join(national_standards_pm2.5, by = "country") %>%
  dplyr::select(-c(natstandard)) %>%
  dplyr::select(objectid_gadm2:whostandard, nat_ann_avg_pm2.5_aqli, dplyr::everything()) %>%
  dplyr::rename(natstandard = nat_ann_avg_pm2.5_aqli)

# replace natstandard == "No national standard", with natstandard == 0, and then changing its data type to "numeric"
aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(natstandard = ifelse(natstandard == "No national standard", NA, natstandard))

aqli_gadm2_collapse_master$natstandard <- as.numeric(aqli_gadm2_collapse_master$natstandard)
  
# adding in the life years lost columns and doing some basic cleaning of column names to bring it into the format that we want, as per the AQLI data dictionary

aqli_gadm2_collapse_master <- aqli_gadm2_collapse_master %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(is.na(natstandard), NA, .x))) %>%
  dplyr::select(objectid_gadm2, iso_alpha3, country, name_1, name_2, population, whostandard, natstandard, everything()) %>%
    dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 2)), .names = "{col}")) %>%
    dplyr::rename_with(~str_replace(.x, "llpp_", ""), dplyr::contains("llpp")) 


### Creating AQLI internal column name version of the CSVs=============================================
# 
# # create a version of old colnames gadm2 file that has waterbodies removed from it
# aqli_gadm2_collapse_master_finalized_wat_adj_internal <- aqli_gadm2_collapse_master %>%
#   anti_join(waterbodies_aqli, by = c("country" = "gadm0", "name_1" = "gadm1", "name_2" = "gadm2"))


# benchmarking
threshold_13 <- Sys.time() 

# write the above (non-geometry version) of gadm2 master file to the collapsed/gadm2 folder (this version incorporates the na pop and missing regions, but only partially incoroporate ViT column name changes request). Full changes version as requested by ViT will be written at the end of this chunk for all 3 levels (both geom and non-geom versions)

aqli_gadm2_collapse_master %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm2_data_path, "[missingAndNAPopRegionsIncorpLLppAddedButViTcolNameChangesPartiallyIncorp]master_global_allyears_gadm2_non_geom.csv"))

#============================shp file generattion area (not necessary to save these separately)=================================

#> add in the geometry column by left joining the above with a color file and then write that to the collapsed/gadm2 folder

# # joining the gadm2 collapse master file with colormap (using objectid as the joining column), to get the geometry column
# aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master %>%
#   dplyr::left_join(colormap %>% rename(objectid_gadm2 = objectid), by = "objectid_gadm2") %>%
#   dplyr::rename(iso_alpha3 = iso_alpha3.x) %>%
#   dplyr::select(-c(iso_alpha3.y, NAME_0, NAME_1, NAME_2))

# # creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm2 collapse master dataset, to avoid any confusions.
# 
# new_col_names_gadm2_collapse_master_geom_ver <- colnames(aqli_gadm2_collapse_master_with_geom) %>%
#   dplyr::as_tibble() %>%
#   dplyr::mutate(
#          col_names_shortened = str_replace(value, "who", "w"), 
#          col_names_shortened = str_replace(col_names_shortened, "nat", "n"), 
#          col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
#          col_names_shortened = str_replace(col_names_shortened, "_", ""), 
#          col_names_shortened = str_replace(col_names_shortened, "objectidgadm2", "objidgadm2"), 
#          col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
#   dplyr::select(col_names_shortened) %>%
#   unlist() %>%
#   as.vector()
# 
# # assigning new colnames to the geom version of the gadm2 collapse master file
# colnames(aqli_gadm2_collapse_master_with_geom) <- new_col_names_gadm2_collapse_master_geom_ver
# 
# # making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file
# aqli_gadm2_collapse_master_with_geom <- aqli_gadm2_collapse_master_with_geom %>%
#   sf::st_as_sf() 
# 
# # write the missing and NA pop region incorporated and ViT column names changes partially incorporated (which will be more than enough for geom version, as it is to be used internally). 
# 
# #This warning was issued while writing the shapefile, need to be resolved: In CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options): GDAL Message 1: One or several characters couldn't be converted correctly from UTF-8 to ISO-8859-1.  This warning # will not be emitted anymore
#   
# aqli_gadm2_collapse_master_with_geom %>%
#   sf::write_sf(str_c(ssd_location_collapsed_gadm2_data_path, "[missingAndNAPopRegionsIncorpButViTcolNameChangesPartiallyIncorp]master_global_allyears_gadm2_geom.shp"))

#============================shp file generattion area (not necessary to save these separately)=================================

# garbage collection
gc()

#-----------


#> collapse the gadm2 file to gadm0 (country) level and write both geom and non-geom versions to their respective folders

# finalizing the gadm0 level dataset
aqli_gadm0_collapse_from_gadm2 <- aqli_gadm2_collapse_master %>%
  dplyr::group_by(country) %>%
  dplyr::mutate(pop_weights = population/sum(population, na.rm = TRUE)) %>%
  dplyr::mutate(across(dplyr::starts_with("pm"), ~(.x*pop_weights), .names = "{col}_pop_weighted")) %>%
  dplyr::summarise(across(dplyr::contains("pop_weighted"), ~(round(sum(.x, na.rm = TRUE), 2)), .names = "avg_{col}"),
                   total_population = sum(population, na.rm = TRUE), objectid_gadm2 = objectid_gadm2[1], iso_alpha3 = iso_alpha3[1], whostandard = whostandard[1], natstandard = natstandard[1]) %>%
  dplyr::ungroup() %>%
  select(objectid_gadm2, iso_alpha3, country, total_population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::mutate(objectid_gadm2 = dplyr::row_number()) %>%
  dplyr::rename(population = total_population, 
                objectid_gadm0 = objectid_gadm2) %>%
  dplyr::rename_with(~str_replace_all(.x, "(_pop_weighted)|(avg_)", ""), dplyr::contains("_pop_weighted")) %>%
    dplyr::mutate(across(starts_with("pm"), (~(.x - whostandard)*aqli_lyl_constant), .names = "llpp_who{col}")) %>%
  dplyr::mutate(across(starts_with("pm"), (~(.x - natstandard)*aqli_lyl_constant), .names = "llpp_nat{col}")) %>%
  dplyr::mutate(across(starts_with("llpp"), ~ifelse(.x < 0, 0, .x))) %>%
  dplyr::mutate(across(starts_with("llpp_nat"), ~ifelse(is.na(natstandard), NA, .x))) %>%
  dplyr::select(objectid_gadm0, iso_alpha3, country, population, whostandard, natstandard, dplyr::everything()) %>%
  dplyr::rename_with(~str_replace(.x, "pm", ""), dplyr::contains("llpp")) %>%
  dplyr::mutate(across(dplyr::matches("pm|llpp"), ~(round(.x, 2)), .names = "{col}")) %>%
  dplyr::mutate(across(matches("pm|llpp"), ~ifelse(population == 0, NA, .x))) %>%
  dplyr::mutate(population = ifelse(population == 0, NA, population)) %>%
  dplyr::rename_with(~str_replace(.x, "llpp_", ""), dplyr::contains("llpp")) 


# write the above (non-geometry version) of gadm0 master file to the collapsed/gadm0 folder (this version incorporates the na pop and missing regions, but only partially incoroporate ViT column name changes request). Full changes version as requested by ViT will be written at the end of this chunk for all 3 levels (both geom and non-geom versions)

aqli_gadm0_collapse_from_gadm2 %>%
  readr::write_csv(str_c(ssd_location_collapsed_gadm0_data_path, "[missingAndNAPopRegionsIncorpLLppAddedButViTcolNameChangesPartiallyIncorp]master_global_allyears_gadm0_non_geom.csv"))

#============================shp file generattion area (not necessary to save these separately)=================================

# # read in the gadm0 shape file (which is collapsed from the colormap) and get it ready for joining with the above summary table
# 
# gadm0_shp_file <- sf::st_read(str_c(shp_files_location, gadm0_shp_file_location, sep = ""))
# 
# gadm0_shp_file_subset <- gadm0_shp_file %>%
#   dplyr::select(-c(NAME_1, NAME_2, objectid, iso_alpha3))
# 
# # add in the geometry column by left joining the above with a country level shape file and then write that to the collapsed/gadm0 folder
# 
# # joining the gadm0 collapse master file with the country level shapefile (using "country/NAME_0" as the joining column), to get the geometry column
# 
# aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2 %>%
#   dplyr::left_join(gadm0_shp_file_subset, by = c("country" = "NAME_0")) %>%
#   sf::st_as_sf()
# 
# # (tbd): creating new shortened colnames, before exporting as a shape file, to be compliant with ESRI shapefile column name restrictions. Note that, when reading the shapefile later on, remember to convert the colnames back to how it appears in the "non-geom" version of the gadm0 collapse master dataset, to avoid any confusions.
# 
# new_col_names_gadm0_collapse_from_gadm2 <- colnames(aqli_gadm0_collapse_from_gadm2_with_geom) %>%
#   dplyr::as_tibble() %>%
#   dplyr::mutate( 
#          col_names_shortened = str_replace(value, "who", "w"), 
#          col_names_shortened = str_replace(col_names_shortened, "nat", "n"), 
#          col_names_shortened = str_replace(col_names_shortened, "standard", "stan"), 
#          col_names_shortened = str_replace(col_names_shortened, "_", ""), 
#          col_names_shortened = str_replace(col_names_shortened, "objectidgadm0", "objidgadm0"), 
#          col_names_shortened = str_replace(col_names_shortened, "isoalpha3", "isoal3")) %>%
#   dplyr::select(col_names_shortened) %>%
#   unlist() %>%
#   as.vector()
# 
# 
# # (tbd): assigning new colnames to the geom version of the gadm0 collapse master file
# colnames(aqli_gadm0_collapse_from_gadm2_with_geom) <- new_col_names_gadm0_collapse_from_gadm2
# 
# # (tbd): making sure to st_as_sf the geom version before exporting to shape file and then exporting the shape file
# 
# aqli_gadm0_collapse_from_gadm2_with_geom <- aqli_gadm0_collapse_from_gadm2_with_geom %>%
#   sf::st_as_sf()
# 
# 
# # write the missing and NA pop region incorporated and ViT column names changes partially incorporated (which will be more than enough for geom version, as it is to be used internally). There were warning related to couple fields whose population values were not successfully written because it was too large a number with respect to the field: "width warning" (GDAL Message 1). Resolve this warning, a screenshot of it is stored in the gadm0 folder under collapsed folder in the ssd.
# aqli_gadm0_collapse_from_gadm2_with_geom %>%
#   sf::write_sf(str_c(ssd_location_collapsed_gadm0_data_path, "[missingAndNAPopRegionsIncorpButViTcolNameChangesPartiallyIncorp]master_global_allyears_gadm0_geom.shp"))

